[{"title":"私有仓库harbor的安装使用","url":"/2022/12/15/docker-harbor/","content":"\n### 环境\n\ncentos7\n\n### 步骤\n\n#### 安装docker\n\n```shell\n#安装常用工具\nyum install -y yum-utils device-mapper-persistent-data lvm2\n#添加阿里云镜像地址\nyum-config-manager \\--add-repo \\http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n#升级系统和安装docker\nyum update -y && yum install -y docker-ce\n#创建目录\nmkdir /etc/docker\n#\ncat > /etc/docker/daemon.json <<EOF\n{\n\"exec-opts\": [\"native.cgroupdriver=systemd\"],\n\"log-driver\": \"json-file\",\n\"log-opts\": {\n\"max-size\": \"100m\"\n},\n\t\"insecure-registries\":[\"https://hw.harbor.com\"]\n}\nEOF\n#创建docker启动\nmkdir -p /etc/systemd/system/docker.service.d\n\n# 重启docker服务\nsystemctl daemon-reload && systemctl restart docker && systemctl enable docker\n```\n\n#### 安装docker-compose\n\n```shell\n#下载docker-compose文件，2.14.0处可更换其他版本\ncurl -L https://get.daocloud.io/docker/compose/releases/download/2.14.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n#赋予执行权限\nchmod a+x /usr/local/bin/docker-compose\n#检测是否安装成功\ndocker-compose --version \n```\n\n![1671018724104](docker-harbor/1671018724104.png)\n\n#### 安装harbor\n\n```shell\n#下载安装包\nhttps://storage.googleapis.com/harbor-releases/release-2.4.0/harbor-offline-installer-v2.4.3.tgz\n#上传至服务器/usr/local/目录,使用下面命令解压\ntar -zxvf harbor-offline-installer-v2.4.3.tgz\n#进入解压出来的目录\ncd harbor\n#复制harbor.yml.tmpl为harbor.yml\ncp harbor.yml.tmpl harbor.yml\n#编辑harbor.yml文件\nvim harbor.yml\nhostname: 你的主机ip\n并注释掉https\n```\n\n![1671019354163](docker-harbor/1671019354163.png)\n\n```shell\n#最后执行harbor目录中的install.sh文件进行安装\n./install.sh\n```\n\n![1671020300110](docker-harbor/1671020300110.png)\n\n最后在浏览器输入你的主机ip进行访问即可。\n\n![1671020256799](docker-harbor/1671020256799.png)\n\n用户名为admin，密码可在harbor.yml配置文件中看到，也可以设自己的密码，更改配置文件要重启一下harbor\n\n```shell\n#重启harbor应用\ndocker-compose stop\ndocker-compose start\n```\n\n\n\n测试，给镜像打标签，并推送至私有仓库\n\n```\nroot@node1 ~]# docker tag nginx:1.14-alpine  hw.harbor.com/library/nginx:1.14-alpine\n[root@node1 ~]# docker push  hw.harbor.com/library/nginx:1.14-alpine\n```\n\n","tags":["k8s"]},{"title":"k8s-kuboard","url":"/2022/11/20/k8s-kuboard/","content":"\n### 安装\n\n```\ndocker run -d \\\n  --restart=unless-stopped \\\n  --name=kuboard \\\n  -p 6017:80/tcp \\\n  -p 10081:10081/tcp \\\n  -e KUBOARD_ENDPOINT=\"http://192.168.1.11:6017\" \\\n  -e KUBOARD_AGENT_SERVER_TCP_PORT=\"10081\" \\\n  -v /home/apps/kuboard/data:/data \\\n  eipwork/kuboard:v3.5.0.3\n```\n\n### 使用\n\n安装完成后，浏览器输入192.168.1.11:6017访问，默认用户名密码admin/Kuboard123\n\n![1663508523660](k8s-kuboard/1671695830886.png)\n\n\n\n","tags":["k8s"]},{"title":"k8s-RBAC","url":"/2022/11/18/k8s-RBAC/","content":"\n### k8s认证\n\n有三种客户端认证的方式\n\n1.http base认证方式，主要通过用户名和密码的方式进行认证。\n\n2.http token认证方式，通过一个token来识别合法用户\n\n3.https证书认证，这种方式是最安全的，也是操作最麻烦的认证方式。HTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者\n\n\n\n\n\n\n\n\n\n### k8s授权\n\n\n\nRBAC:角色的访问权限控制，顶级资源包含四个：Role，ClusterRole，RoleBinding，ClusterROleBinding\n\n\n\nRole:只作用于同一个命名空间，namespace隔离，没有拒绝规则，只有允许规则\n\nClusterRole:作用整个集群，所有的namespace都起作用。\n\nRolebonding:作用于命名空间内，将Role，ClusterRole绑定到user，Group、ServiceAccount\n\nClusterRoleBinding：作用于整个集群。\n\n相关指令\n\n```shell\n#列出所有角色\nkubectl get role --all-namespaces\n#查看角色的详细配置\nkubectl get role -n namespace -oyaml\n```\n\n\n\n案例：\n\n用useradd创建一个用户devuser，dev命名空间可允许访问用户\n\n```\nuseradd devuser\n```\n\n创建用户json信息\n\n```\ncat > devuser-csr.json <<EOF\n{\n  \"CN\": \"devuser\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"guangzhou\",\n      \"L\": \"gaungzhou\",\n      \"O\": \"k8s\",\n      \"OU\": \"System\"\n    }\n  ]\n}\nEOF\n```\n\n下载证书生成工具\n\n```\nwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\nmv cfssl_linux-amd64 /usr/local/bin/cfssl\n\nwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nmv cfssljson_linux-amd64 /usr/local/bin/cfssljson\n\nwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64\nmv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo\n#然后赋予执行权限\nchmod a+x *\n#生成证书文件\ncfssl gencert -ca=ca.crt -ca-key=ca.key  -profile=kubernetes devuser-csr.json | cfssljson -bare devuser\n\n```\n\n设置集群参数\n\n```\n#配置环境变量\nexport KUBE_APISERVER=\"https://192.168.1.11:6643\"\n\nkubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=devuser.kubeconfig\n```\n\n设置客户端认证参数\n\n```\nkubectl config set-credentials devuser --client-certificate=/etc/kubernetes/pki/ca.crt --client-key=/etc/kubernetes/pki/ --embed-certs=true --kubeconfig=devuser.kubeconfig\n```\n\n设置上下文参数\n\n```\nkubectl config set-context kubernetes --cluster=kubernetes --user=devuser --namespace=dev --kubeconfig=devuser.kubeconfig\n```\n\n设置默认上下文，将用户绑定至某个名称空间\n\n```\nkubectl create rolebinding devuser-admin-binding --clusterrole=admin --user=devuser --namespace=dev\n\n#将配置文件放到用户目录下\ncp devuser.kubeconfig /home/devuser/.kube/\nchown devuser:devuser /home/devuser/.kube/devuser.kubeconfig\n#更改名字\nmv devuser.kubeconfig config\n#设置上下文\nkubectl config use-context kubernetes --kubeconfig=config\n```\n\n","tags":["k8s"]},{"title":"k8s调度任务","url":"/2022/10/19/k8s-schedule/","content":"\n### cronjob\n\n和linux上的定时任务是一样的，用于定时执行某些任务,存在某些时差\n\n\n\n案例\n\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"* * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: nginx:1.20.0\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n```\n\n\n\n\n\n### 污点和容忍\n\n污点：给某个服务器节点打上污点，如果应用不能容忍这个污点就不会部署在这节点上，例如master节点不建议部署pod。每个节点可以打上多个污点。\n\n\n\n\n\n如何添加污点：\n\nkubectl taint node node名 key=value:污点三个可选值\nNoSchedule : 一定不被调度\nPreferNoSchedule ： 尽量不被调度\nNoExecute : 不会调度，并且还会驱逐Node已有Pod\n\n例句：\n\n```shell\nkubectl taint node node1 key1=value1:NoSchedule\n```\n\nyaml文件中添加容忍如下：\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-volume-nfs\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-volume-nfs\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: nginx-volume-nfs\n    spec:\n      containers:\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-volume-nfs\n          ports:\n          - containerPort: 80\n          volumeMounts:\n          - name: test-volume\n            mountPath: /tmp\n      volumes:\n      - name: test-volume\n        nfs:\n          server: 192.168.1.11  #nfs的服务端地址\n          path: /opt/nfs        #共享的路径\n      tolerations:                              #添加容忍\n      - key: \"key1\"\n        operator: \"Equal\"\n        value: \"value1\"\n        effect: \"NoSchedule\"\n```\n\n\n\n如果node节点有多个Tain，每个Tain都需要容忍才可以部署上去。\n\n如果只写容忍的key不写value，只要符合key的节点都可以部署上去，如下所示\n\n```yaml\ntolerations:                              #添加容忍\n      - key: \"key1\"\n        operator: \"Equal\"\n        effect: \"NoSchedule\"\n```\n\n\n\n### initContainer\n\n初始化容器：在应用容器启动前做的一些初始化操作\n\n和poststart的区别：\n\nInitContainer：不依赖主应用的环境，可以有更高的权限和更多的工具，一定会在主应用启动之前完成\n\nPostStart：依赖主应用的环境，而且并不一定先于Command运行。\n\n\n\n案例\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! && sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]\n```\n\n可以通过下面的命令查看日志\n\n```shell\nkubectl logs -f myapp-pod -c init-myservice\n```\n\n\n\n### affinity\n\n注意：如果节点超过1000不建议用Affinity，因为调度计算会消耗大量时间，当然官方也有优化方案\n\n\n\n主要分为下面三类：\n\n#### 节点亲和力\n\n```\nodeAffinity：节点亲和力\nrequiredDuringSchedulingIgnoredDuringExecution: 硬亲和力，即支持必须部署在指定的节点上，也支持必须不部署在指定节点上\npreferredDuringSchedulingIgnoredDuringExecution: 软亲和力，尽量部署在满足条件的节点上，或者是尽量不用部署在被匹配的节点上\n```\n\n匹配表达式：\n\n- In：label 的值在某个列表中\n- NotIn：label 的值不在某个列表中\n- Gt：label 的值大于某个值\n- Lt：label 的值小于某个值\n- Exists：某个 label 存在\n- DoesNotExist：某个 label 不存在\n\n案例\n\n\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-affinity\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-affinity\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: nginx-affinity\n    spec:\n      containers:\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-affinity\n          ports:\n          - containerPort: 80\n          volumeMounts:\n          - name: test-volume\n            mountPath: /tmp\n      volumes:\n      - name: test-volume\n        hostPath:\n          path: /opt\n          type: Directory\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:  #必须满足\n            nodeSelectorTerms:\n            - matchExpressions:         #下面这些条件是可以单独拿出来匹配出目标\n              - key: disktype        #标签disktype\n                operator: In         #等于\n                values:\n                - ssd                # ssd\n          preferredDuringSchedulingIgnoredDuringExecution: #尽量满足\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: processor      #labels key  \n                operator: In        #等于\n                values:\n                - gpu               # gpu\n\n```\n\n\n\n#### pod亲和力\n\n使用场景，假如你的数据库部署在node01节点上，如果你也想将后端应用也和数据库部署在同一个节点上，这时就用到pod亲和力来实现。\n\n同样使用node亲和力的表达式\n\n\n\n```\n    requiredDuringSchedulingIgnoredDuringExecution: 将a应用和b应用部署在一块\n    preferredDuringSchedulingIgnoredDuringExecution:  尽量将a应用和b应用部署在一块\n```\n\n案例\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-affinity\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-affinity\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: nginx-affinity\n    spec:\n      containers:\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-affinity\n          ports:\n          - containerPort: 80\n          volumeMounts:\n          - name: test-volume\n            mountPath: /tmp\n      volumes:\n      - name: test-volume\n        hostPath:\n          path: /opt\n          type: Directory\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: meme\n                operator: NotIn\n                values:\n                - hh\n#          preferredDuringSchedulingIgnoredDuringExecution:\n#          - labelSelector:\n#              matchExpressions:\n#              - key: name\n#                operator: In\n#                values:\n#                - hh\n            namespace: \n              - kube-system #如果配置就在这个空间里面搜索，不配置就是全局搜索\n            topologyKey: kubernetes.io/hostname\n```\n\n#### pod反亲和力\n\n\n\n\n\n\n\n### topologyKey\n\ntopologyKey可以看做一个范围，当我们部署应用时，我们只希望应用在这个范围里面部署，则可以设置该值。\n\n\n\n官方解释：\n\n如果该X已经在运行一个或多个满足规则Y的Pod，则该Pod应该（或者在非亲和性的情况下不应该）在X中运行\n\nY表示为LabelSelector规则\nX是一个拓扑域，例如节点，机架，云提供者区域，云提供者区域等。您可以使用topologyKey这是系统用来表示这种拓扑域的节点标签的密钥\n\n\n\n案例\n\n需求：当前有两个机房（ beijing，shanghai），需要部署一个nginx产品，副本为两个，为了保证机房容灾高可用场景，需要在两个机房分别部署一个副本\n\n```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nginx-affinity-test\nspec:\n  serviceName: nginx-service\n  replicas: 2\n  selector:\n    matchLabels:\n      service: nginx\n  template:\n    metadata:\n      name: nginx\n      labels:\n        service: nginx\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: service\n                operator: In\n                values:\n                - nginx\n            topologyKey: zone\n      containers:\n      - name: nginx\n        image: contos7:latest\n        command:\n        - sleep\n        - \"360000000\"\n```\n\n说明：两个node上分别有zone标签，来标注自己属于哪个机房，topologyKey定义为zone，pod所以在调度的时候，会根据node上zone标签来区分拓扑域，当前用的上 反亲和性调度 根据拓扑纬度调度，beijing机房调度完一个pod后，然后控制器判断beijing 拓扑域已经有server=nginx标签的pod，就在下一个拓扑域的node上调度了.\n\n\n\n### 临时容器\n\n因为我们平时启动的pod，里面运行的容器的镜像都是尽可能的小，很多工具都是没有的，用到的一些网络排查工具也是没有安装上，这时就要启动一个临时的容器安装所需要的工具。\n\n\n\n开启临时容器功能，所有节点都要配置\n\n```shell\nvi /usr/lib/systemd/system/kube-apiserver.conf\n--feature-gates=EphemeralContainers=true\n\nvi /usr/lib/systemd/system/kube-controller-manager.conf\n--feature-gates=EphemeralContainers=true\n\nvi /usr/lib/systemd/system/kube-scheduler.conf\n--feature-gates=EphemeralContainers=true\nvi /usr/lib/systemd/system/kube-proxy.conf\n--feature-gates=EphemeralContainers=true\n\nvi /etc/kubernetes/kubelet-conf.yml\nfeatureGates:\n  EphemeralContainers: true\n```\n\n重启所有节点\n\n```shell\nsystemctl daemon-reload\nsystemctl restart kubelet\n```\n\n\n\n创建json文件，开始使用\n\n```json\n{\n   \"apiVersion\": \"v1\",\n    \"kind\": \"EphemeralContainers\",\n    \"metadata\": {\n           \"name\": \"tomcat-test\"      //这里是pod的容器名称，注入到哪里\n    },\n   \"ephemeralContainers\": [{\n       \"command\": [\n           \"sh\"\n        ],\n       \"image\": \"busybox\",\n       \"imagePullPolicy\": \"IfNotPresent\",\n        \"name\":\"debugger\",            //临时容器名称\n       \"stdin\": true,\n        \"tty\":true,\n       \"targetContainerName\": \"tomcat-java\",\n       \"terminationMessagePolicy\": \"File\"\n    }]\n}\n```\n\njson文件编辑好以后，运行下面的命令完成注入\n\n```shell\nkubectl replace --raw /api/v1/namespaces/default/pods/tomcat-test/ephemeralcontainers -f a.json\n\n#注意是属于哪个namespaces，上面语句是默认的namespaces default\n```\n\n查看pod详情，看临时容器的状态为running则成功\n\n```shell\nkubectl describe pod podname\n```\n\n进入临时容器\n\n```shell\nkubectl exec -it podname -c 临时容器名称  -- sh\n```\n\n\n\n\n\n相关指令\n\n```shell\n#查看定时任务\nkubectl get cj\n#查看日志\nkubectl logs -f pod名称\n#查看污点\nkubectl describe node node1 | grep Taints\n#添加污点\nkubectl taint node node1 key1=value1:NoSchedule\n#删除污点\nkubectl taint node node1 key1:NoSchedule-\n#查看标签\nkubectl get nodes --show-labels\n#添加标签\nkubectl label nodes kube-node label_name=label_value\n#删除标签\nkubectl label nodes k8s-test01 gpu-\n#修改标签\nkubectl label nodes k8s-test01 gpu=false --overwrite\n```\n\n","tags":["k8s"]},{"title":"k8s数据共享volume","url":"/2022/10/19/k8s-volume/","content":"\n### hostPath\n\n将宿主机本地的文件或者目录挂载到pod容器上。\n\n\n\n案例：\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-volume\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-volume\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: nginx-volume\n    spec:\n      containers:\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-volume\n          ports:\n          - containerPort: 80\n          volumeMounts:\n          - name: test-volume\n            mountPath: /tmp\n      volumes:\n      - name: test-volume\n        hostPath:\n          path: /opt\n          type: Directory\n```\n\n在容器启动前一定要确保宿主机下有要挂载的目录，然后通过kubectl create -f 文件名创建deployment。\n\nhostPath的类型主要有：\n\n| DirectoryOrCreate | 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 |\n| ----------------- | ------------------------------------------------------------ |\n| Directory         | 在给定路径上必须存在的目录。                                 |\n| FileOrCreate      | 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。 |\n| File              | 在给定路径上必须存在的文件。                                 |\n\n\n\n\n\n### emptyDir\n\n两个pod之间的数据共享，当pod被删除的时候，数据也会别删除。\n\n\n\n案例\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-emptydir\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-emptydir\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: nginx-emptydir\n    spec:\n      containers:\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-emptydir\n          ports:\n          - containerPort: 80\n          volumeMounts:\n          - name: test-emptydir\n            mountPath: /tmp\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-emptydir2\n          command:\n          - sh\n          - -c\n          - sleep 3600\n          ports:\n          - containerPort: 81\n          volumeMounts:\n          - name: test-emptydir\n            mountPath: /opt\n      volumes:\n      - name: test-emptydir\n        emptyDir: {}\n```\n\n\n\n### NFS\n\n#### 1.安装nfs\n\n服务端安装软件包\n\n```\nyum -y install nfs-utils rpcbind\n```\n\n编辑/etc/exports文件，添加下面的内容，ip段换和共享路径成自己的。\n\n```\n/opt/nfs  192.168.1.0/24(rw,no_root_squash,no_all_squash,sync,anonuid=501,anongid=501)\n```\n\n启动服务，添加自动启动\n\n```\nsystemctl start nfs\nsystemctl enable nfs\nsystemctl start rpcbind\nsystemctl enable rpcbind\n```\n\n客户端安装服务\n\n```\nyum -y install nfs-utils\n```\n\n客户端共享服务端目录\n\n```\nmount -t nfs 192.168.1.11:/opt/nfs /mnt\n```\n\n\n\n#### 2.pod使用nfs\n\n创建yaml文件，配置如下：\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-volume-nfs\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-volume-nfs\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: nginx-volume-nfs\n    spec:\n      containers:\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-volume-nfs\n          ports:\n          - containerPort: 80\n          volumeMounts:\n          - name: test-volume\n            mountPath: /tmp\n      volumes:\n      - name: test-volume\n        nfs:\n          server: 192.168.1.11  #nfs的服务端地址\n          path: /opt/nfs        #共享的路径\n```\n\n创建deployment\n\n```shell\nkubectl create -f 文件名\n```\n\n\n\n### PersistentVolume\n\n\n\n![202111110117733](/k8s-volume/202111110117733.jpeg)\n\n\n\n\n\n创建一个静态pv，使用nfs资源\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv0003\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /tmp\n    server: 172.17.0.2\n```\n\n### PersistentVolumeClaim\n\n创建pvc\n\n\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myclaim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi                  #大小不能超过pv的大小\n  storageClassName: slow            #此处名称要和pv的一致\n```\n\n\n\n创建pod引用pvc\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: myfrontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: mypd\n  volumes:\n    - name: mypd\n      persistentVolumeClaim:\n        claimName: myclaim\n```\n\n","tags":["k8s"]},{"title":"k8s配置管理","url":"/2022/10/04/k8s-config/","content":"\n### ConfigMap\n\n官方地址：https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/\n\n 一般用configMap去管理一些配置文件、环境变量信息，将配置和pod分开，更易于配置文件的更改和管理。\n\nsecret：secret更倾向于于存储和共享敏感、加密的配置信息。\n\n\n\n\n\n#### 创建方式\n\n从目录的方式创建configMap,如下官网的案例\n\n```shell\n# 创建本地目录\nmkdir -p configure-pod-container/configmap/\n\n# 将示例文件下载到 `configure-pod-container/configmap/` 目录\nwget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties\nwget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties\n\n# 创建 configmap\nkubectl create configmap game-config --from-file=configure-pod-container/configmap/\n#查看创建的configMap\nkubectl get cm game-config -o yaml\n```\n\n\n\n从文件的方式创建configMap\n\n```shell\n#不指定名字，则使用文件的名字\nkubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties\n#指定别名为game-self\nkubectl create configmap game-config-2 --from-file=game-self=configure-pod-container/configmap/game.properties\n```\n\n根据字面值创建 ConfigMap，这个方式用的少，相当于直接使用命令进行定义\n\n```\nkubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm\n```\n\n\n\n#### 引用文件\n\n\n\n通过pod中envFrom定义，将configmap中所有的key-value生成为容器的环境变量，引用了上面创建的game-config。\n\n\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata: \n  name: cm-test-pod\nspec:\n  containers:\n  - name: cm-test\n    image: nginx:1.15\n    envFrom:\n    - configMapRef:\n        name: game-config\n```\n\n容器启动成功后使用下面命令即可查看到效果\n\n```\nkubectl exec -it cm-test-pod -- bash -c \"env\"\n```\n\n![1665287015068](D:\\个人文件\\Linux学习\\我的\\typora\\1665287015068.png)\n\n\n\n相关指令\n\n```shell\n#查看创建的configMap\nkubectl get cm game-config -o yaml\n#查看容器内的环境变量\nkubectl exec -it cm-test-pod -- bash -c \"env |grep app\"\n```\n\n\n\n### secret\n\n用于保存敏感信息，密码等信息。\n\n创建和使用方式都和configMap差不多。\n\n\n\nimagePullSecret:pod拉取私有镜像仓库时使用的用户密码，里面的账号信息，会传递给kubectl，然后kubectl就拉取有账号密码的仓库镜像。\n\n\n\n创建imagePullSecret\n\n```\nkubectl create secret docker-registry myregistrykey --docker-server=DUMMY_SERVER \\\n          --docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \\\n          --docker-email=DUMMY_DOCKER_EMAIL      \n myregistrykey     #名称\n --docker-server   #dockerhub仓库\n --docker-username #用户名\n --docker-password #密码\n --docker-email    #邮箱\n \n #查看创建的secret\n kubectl get secret\n```\n\n\n\n### subPath解决目录被覆盖的问题\n\n如何解决configMap覆盖掉文件夹下其他的文件\n\n\n\n案例\n\n\n\n1.创建configMap\n\n\n\n编辑nginx.conf文件\n\n```\nuser  root;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n}\n```\n\n执行下面命令\n\n```shell\nkubectl create cm nginx-cm --from-file=nginx.conf\n#查看创建明细\nkubectl describe cm nginx-cm\n```\n\n\n\n2.编辑deployment的yaml文件\n\n配置如下：\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-subpath\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-subpath\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: nginx-subpath\n    spec:\n      containers:\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-subpath\n          ports:\n          - containerPort: 80\n          volumeMounts:\n          - name: nginx-config                 #容器内的名称\n            mountPath: /etc/nginx/nginx.conf   #容器内挂载路径\n            subPath: nginx.conf\n      volumes:\n        - name: nginx-config                   #与上面的name一致\n          configMap:\n            name: nginx-cm                     #configMap名称\n            items:\n            - key: nginx.conf\n              path: nginx.conf\n```\n\n执行命令创建deployment\n\n```shell\nkubectl create -f nginx-subpath.yaml\n```\n\n\n\n\n\n### configMap热更新\n\n\n\n#### 问题1：\n\n如果以subPath的形式挂载，pod无法感知configMap和secret的变化。\n\n解决方法：\n\n将configMap挂载到pod中的临时目录，并在pod中使用软连接到真正的目录中，\n\n案例：挂载两个configMap的路径进行对比，/mnt/路径是没用使用subPath的，所以更新configMap时会感知到并进行更新，在用软连接前使用postStart在容器启动之前执行命令删除原目录的文件。\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-subpath\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-subpath\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: nginx-subpath\n    spec:\n      containers:\n        - image: nginx:1.20.0\n          imagePullPolicy: Always\n          name: nginx-subpath\n          ports:\n          - containerPort: 80\n          volumeMounts:\n          - name: nginx-config                 #容器内的名称\n            mountPath: /etc/nginx/nginx.conf   #容器内挂载路径\n            subPath: nginx.conf\n          - name: nginx-config2                #容器内的名称\n            mountPath: /mnt/   #容器内挂载路径\n      volumes:\n        - name: nginx-config                   #与上面的name一致\n          configMap:\n            name: nginx-cm                     #configMap名称\n            items:\n            - key: nginx.conf\n              path: nginx.conf\n        - name: nginx-config2                   #与上面的name一致\n          configMap:\n            name: nginx-cm                     #configMap名称\n```\n\n\n\n\n\n\n\n#### 问题:2：\n\n直接使用kubectl edit cm configMap对比较大的文件是无法编辑的。\n\n解决方法：\n\n1.使用图形化界面工具\n\n2.将configMap导出成yaml文件。\n\n3.使用下面的命令进行更新。\n\n```\nkubectl create cm nginx-cm --from-file=nginx.conf --dry-run -oyaml | kubectl replace -f-\n```\n\n\n\n\n\n相关指令\n\n```shell\n#查看目录下的文件\nkubectl exec -it podname  -- ls /etc/nginx\n#更新configMap\nkubectl create cm nginx-cm --from-file=nginx.conf --dry-run -oyaml | kubectl replace -f-\n```\n\n","tags":["k8s"]},{"title":"k8s服务发布","url":"/2022/09/30/k8s-service/","content":"\n\n\n### Lable&selector\n\nlable:对各种资源进行分类、分组，添加一个具有特别属性的一个标签。\n\nSelector:通过一个过滤的语法进行查找到对应的标签的资源。\n\n\n\n注意：使用命令更改label只是一时的，当更新或者混滚时还是会被重置，正确的方法是改deployment的资源文件，将旧的更改名字，然后修改label，再启动新的，重新指定service到新的label,再删除旧的。\n\n\n\n```shell\n#设置node的label的名称为subnet7\nkubectl label node k8s-node02 region=subnet7\n\n#查询label为asubnet7的node\nkubectl get node -l region=subnet7\n\n#显示label信息\nkubectl get pod --show-labels\n\n#查看app=xxx的pod\nkubectl get pod -A -l app=xxxx\n\n#运行程序\nkubectl run busybox --image=busybox:1.28 -n kube-test\n\npod添加标签\nkubectl label pod busybox app=busybox -n kube-test\n\n#删除label\nkubectl label po busybox app- -n kube-test\n\n#修改label\nkubectl label pod busybox app=busybox2 -n kube-public --overwrite\n\n#查看labels包含name1,name2的pod\nkubectl get pod -A -l 'k8s-app in (name1,name2)'\n\n#查看app=nginx,version不等于v1的pod\nkubectl get pod -l version!=v1,app=nginx\n```\n\n\n\n### service\n\nservice可以简单的理解为逻辑上的一组pod。一种可以访问pod的策略，而且其他pod可以通过这个service访问到这个service代理的pod，相对于pod而言，它会有一个固定的名称，一旦创建就固定不变。\n\n![1664547819285](D:\\个人文件\\Linux学习\\我的\\typora\\1664547819285.png)\n\n#### 创建busybox\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\n```\n\n\n\n#### 创建一个service\n\n```shell\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx-svc\n  name: nginx-svc\nspec:\n  ports:\n  - name: http  #service端口的名称\n    port: 80    #service自己端口 \n    protocol: TCP\n    targetPort: 80 #后端应用的端口\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 443\n  selector:\n    app: nginx-test  #这个取你nginx的pod对应标签的名称\n  sessionAffinity: None\n  type: ClusterIP\n```\n\n相关指令\n\n```shell\n#跨匿名空间的访问方式\ncurl http://nginx-svc.default\n#进入容器指令\nkubectl exec -it busybox -- sh\n#获取service的endpoint信息\nkubectl get ep\n#查看ep yaml\nkubectl get ep nignx-svc -oyaml\n```\n\n#### 使用service访问k8s外部应用(ip地址)\n\n原理：更改service和endpoit的指向，将service的selector去掉，不去配对k8s里的pod应用\n\n\n\nservice配置文件\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx-svc-external\n  name: nginx-svc-external\nspec:\n  ports:\n  - name: http  #service端口的名称\n    port: 80    #service自己端口 \n    protocol: TCP\n    targetPort: 80 #后端应用的端口\n  sessionAffinity: None\n  type: ClusterIP\n```\n\nendpoit配置文件\n\n```yaml\napiVersion: v1\nkind: Endpoints\nmetadata:\n  labels:\n    app: nginx-svc-external #要与service配置一致\n  name: nginx-svc-external  #要与service配置一致\n  namespace: default\nsubsets:\n- addresses:\n  - ip: 10.50.140.101       #外部ip地址\n  ports:\n  - name: http              #要与service配置一致\n    port: 80                #要与service配置一致\n    protocol: TCP           #要与service配置一致\n```\n\n#### 使用service访问k8s外部应用(域名)\n\n原理：只需要创建service，配置如下\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx-svc-externalname\n  name: nginx-svc-externalname\nspec:\n  type: ClusterIP\n  externalName: www.baidu.com\n```\n\n#### service类型\n\nClusterIP：在集群内部使用，也是默认值。\n\nExternalName：通过返回定义的CNAME别名。\n\nNodePort：在所有安装了kube-proxy的节点上打开一个端口，此端口可以代理至后端Pod，\n\n然后集群外部可以使用节点的IP地址和NodePort的端口号访问到集群Pod的服务。NodePort端口范围默认是30000-32767。（性能不好，对外建议使用ingress）\n\nloadBalancer：使用云提供商的负载均衡器公开服务。\n\n### Ingress\n\n也是k8s的资源类型，ingress用于实现用域名的方式访问k8s内部应用。\n\ningress官方地址：https://kubernetes.github.io/ingress-nginx/examples/rewrite/\n\nhelm官方地址https://helm.sh/docs/intro/install/\n\n#### ingress安装\n\n下载helm安装包\n\n```\n#下载安装包\nhttps://get.helm.sh/helm-v3.10.0-linux-amd64.tar.gz\n#解压安装包\ntar -zxvf helm-v3.0.0-linux-amd64.tar.gz\n#最后执行下面命令即可\nmv linux-amd64/helm  /usr/local/bin/helm\n```\n\n安装ingress\n\n```shell\n#添加仓库地址\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n#更新仓库地址\nhelm repo update\n#查询版本\nhelm search repo ingress-nginx\n#下载ingress\nhelm pull ingress-nginx/ingress-nginx\n#解压\ntar -zxvf ingress-nginx-4.2.5.tgz\n#进入解压包修改values.yaml配置\n\nrepository: registry.cn-hangzhou.aliyuncs.com #修改\nimage: kubelibrary/ingress-nginx-controller\ndnsPolicy: ClusterFirstWithHostNet #修改\nhostNetwork: true #修改\nkind: DaemonSet #修改\nnodeSelector:\n    kubernetes.io/os: linux\n    ingress: \"true\" #添加\ntargetPorts:\n      http: http\n      https: https\n\n    type: ClusterIP #修改\n    \n\nadmissionWebhooks: \npatch:\n      enabled: true\n      image:\n        registry: registry.cn-hangzhou.aliyuncs.com #修改\n        image: kubelibrary/kube-webhook-certgen\n        \n#安装\nkubectl create ns ingress-nginx\nhelm install ingress-nginx -n ingress-nginx .\n\n#在master03使用ingress，先打标签\nkubectl label node node01 ingress=true\n\n```\n\n创建ingress，vim ingress.yaml\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n  name: example\nspec:\n  rules:\n  - host: foo.bar.com        #外部访问的域名\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nginx-svc  #对应service的名称\n            port: \n              number: 80     #对应service端口号      \n```\n\n\n\n执行创建语句\n\n```shell\nkubectl create -f ingress.yaml\n\n#查看ingress是否已经起来\nkubectl get ingress\n```\n\n在浏览器输入foo.bar.com即可访问","tags":["k8s"]},{"title":"k8s资源调度","url":"/2022/09/28/k8s-resources/","content":"\n## Deployment概念\n\n用于部署无状态的服务，这个最常用的控制器，一般用于管理维护企业内部无状态的微服务，比如configserver、zuul、springboot。他可以管理多个副本的pod实现无缝迁移、自动扩容缩容、自动灾难恢复、一键回滚等功能。\n\n\n\ndeployment配置文件介绍\n\n```\napiVersion: apps/v1  # 指定api版本，此值必须在kubectl api-versions中 \nkind: Deployment  # 指定创建资源的角色/类型  \nmetadata:  # 资源的元数据/属性\n  name: demo  # 资源的名字，在同一个namespace中必须唯一\n  namespace: default # 部署在哪个namespace中\n  labels:  # 设定资源的标签\n    app: demo\n    version: stable\nspec: # 资源规范字段\n  replicas: 1 # 声明副本数目\n  revisionHistoryLimit: 3 # 保留历史版本\n  selector: # 选择器\n    matchLabels: # 匹配标签\n      app: demo\n      version: stable\n  strategy: # 策略\n    rollingUpdate: # 滚动更新\n      maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数\n      maxUnavailable: 30% # 示在更新过程中能够进入不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n    type: RollingUpdate # 滚动更新策略\n  template: # 模版\n    metadata: # 资源的元数据/属性\n      annotations: # 自定义注解列表\n        sidecar.istio.io/inject: \"false\" # 自定义注解名字\n      labels: # 设定资源的标签\n        app: demo\n        version: stable\n    spec: # 资源规范字段\n      containers:\n      - name: demo # 容器的名字  \n        image: demo:v1 # 容器使用的镜像地址  \n        imagePullPolicy: IfNotPresent # 每次Pod启动拉取镜像策略，三个选择 Always、Never、IfNotPresent\n                                      # Always，每次都检查；Never，每次都不检查（不管本地是否有）；IfNotPresent，如果本地有就不检查，如果没有就拉取\n        resources: # 资源管理\n          limits: # 最大使用\n            cpu: 300m # CPU，1核心 = 1000m\n            memory: 500Mi # 内存，1G = 1024Mi\n          requests:  # 容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行\n            cpu: 100m\n            memory: 100Mi\n        livenessProbe: # pod 内部健康检查的设置\n          httpGet: # 通过httpget检查健康，返回200-399之间，则认为容器正常\n            path: /healthCheck # URI地址\n            port: 8080 # 端口\n            scheme: HTTP # 协议\n            # host: 127.0.0.1 # 主机地址\n          initialDelaySeconds: 30 # 表明第一次检测在容器启动后多长时间后开始\n          timeoutSeconds: 5 # 检测的超时时间\n          periodSeconds: 30 # 检查间隔时间\n          successThreshold: 1 # 成功门槛\n          failureThreshold: 5 # 失败门槛，连接失败5次，pod杀掉，重启一个新的pod\n        readinessProbe: # Pod 准备服务健康检查设置\n          httpGet:\n            path: /healthCheck\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 5\n        #也可以用这种方法  \n        #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常  \n        #  command:  \n        #    - cat  \n        #    - /tmp/health  \n        #也可以用这种方法  \n        #tcpSocket: # 通过tcpSocket检查健康 \n        #  port: number\n        ports:\n          - name: http # 名称\n            containerPort: 8080 # 容器开发对外的端口\n            protocol: TCP # 协议\n      imagePullSecrets: # 镜像仓库拉取密钥\n        - name: harbor-certification\n      affinity: # 亲和性调试\n        nodeAffinity: # 节点亲和力\n          requiredDuringSchedulingIgnoredDuringExecution: # pod 必须部署到满足条件的节点上\n            nodeSelectorTerms: # 节点满足任何一个条件就可以\n            - matchExpressions: # 有多个选项，则只有同时满足这些逻辑选项的节点才能运行 pod\n              - key: beta.kubernetes.io/arch\n                operator: In\n                values:\n                - amd64\n```\n\n\n\n### Deployment创建方式\n\n手动创建\n\n\n\nyaml文件创建\n\n\n\n\n\n### Deployment的更新\n\n\n\n可以使用下面指令更新\n\n```\nkubectl set image deployment nginx nginx=nginx:1.15.3 --record\n```\n\n查看更新过程\n\n```\nkubectl rollout status deploy nginx\n```\n\n\n\n### Deployment的回滚\n\n查看历史升级命令\n\n```\nkubectl rollout history deployment nginx\n```\n\n回滚到上一个版本命令\n\n```\nkubectl rollout undo deployment nginx\n```\n\n回滚到指定版本的操作\n\n```shell\n#查看指定版本的信息\nkubectl rollout history deployment nginx --revision=5\n#回滚到指定版本命令\nkubectl rollout undo deployment nginx --to-revision=5\n```\n\n\n\n### 扩缩容\n\n```\nkubectl scale --replicas=3 deploy nginx\n```\n\n\n\n## 有状态应用管理StatefulSet\n\n\n\n案例一\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  serviceName: \"nginx\"\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: nginx\n        image: nginx:1.20.0\n        ports:\n        - containerPort: 80\n          name: web\n```\n\n扩容指令\n\n```shell\nkubectl scale --replicas=3 sts web\n```\n\n查看pod实时动态\n\n```\nkubectl get pod -l app=nginx -w\n```\n\n更新策略\n\n```yaml\nupdateStrategy:\nrollingUpdate:\npartition: 0\ntype: RollingUpdate或onDelete\n```\n\nRollingUpdate：从下网上逐个更新\n\nonDelete：删除原来的pod才触发更新\n\npartition：例如为2时，小于2的不会更新\n\n\n\n删除操作\n\n1.级联删除：删除sts，pod也会跟着删除\n\n```\nkubectl delete sts web\n```\n\n2.非级联删除：删除sts，pod不会跟着删除\n\n```\nkubectl delete sts web --cascade=false\n```\n\n## Daemonset\n\nDaemonSet 是一个确保全部或者某些节点上必须运行一个 Pod的工作负载资源，当有节点加入集群时， 也会为他们新增一个 Pod。\n\n\n\n案例\n\n```yaml\napiVersion: apps/v1 \nkind: DaemonSet \nmetadata: \n  name: my-daemonset\nspec: \n  selector: \n    matchLabels: \n      app: my-daemon\n  template: \n    metadata: \n      labels: \n        app: my-daemon\n    spec: \n      containers: \n        - name: daemonset-container \n          image: nginx:1.20.0\n          ports: \n          - containerPort : 80\n```\n\n### 更新和回滚\n\n```shell\n#更新\nkubectl set image ds nginx nginx=nginx:1.15.3 --record\n#查看历史更新记录\nkubectl rollout history ds nginx -n xxxx\n#指定版本回滚\nkubectl rollout history ds nginx --revision=2 -n xxx\n```\n\n### HPA自动扩缩容\n\nHPA(Horizontal Pod Autoscaler)根据pod，cpu，内存等选项值进行扩缩容，不适用与DaemonSet，前提条件必须安装Metrics Server，配置 requests 参数值\n\n案例\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-test\n  labels:\n    k8s-app: nginx\n    traffic-type: external\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-test\n  template:\n    metadata:\n      labels:\n        app: nginx-test\n    spec:\n      containers:\n      - name: nginx-test\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n#        startupProbe:\n#          httpGet:\n#            path: /api/successStart\n#            port: 80\n#          tcpSocket: \n#            port: 80\n#          failureThreshold: 1\n#          periodSeconds: 10\n#          successThreshold: 1\n#          timeoutSeconds: 2\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          failureThreshold: 1\n#          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 170Mi\n          requests:\n            cpu: 10m\n            memory: 70Mi\n        livenessProbe:\n          exec:\n            command:\n            - ls\n          failureThreshold: 1\n#          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n```\n\n用下面命令创建当cpu百分比大于10的时候，扩容至10个副本\n\n```shell\nkubectl autoscale deployment hpa-demo --cpu-percent=20 --min=1 --max=5\n```\n\n查看cpu占用情况\n\n```\nkubectl top po -n kube-system\n\n#查看hpa\nkubectl get hpa\n```\n\n","tags":["k8s"]},{"title":"k8s常用指令","url":"/2022/09/28/k8s-command/","content":"\n```shell\n#查看命名空间有哪些\nkubectl get ns\n#查看pod状态\nkubectl get pod -n xxxxx -owide\n#查看deployment的配置文件\nkubectl get pod calico-node-zsn5t  -n kube-system -oyaml\n#编辑deployment的配置文件\nkubectl edit deploy coredns -n kube-system\n#查看pod得详情\nkubectl describe pod nginx\n#创建pod命令\nkubectl create -f pod.yaml\n#进入容器内部\nkubectl exec -it cm-test-pod -- bash -c \"env |grep app\"\n#查看容器的日志\nkubectl logs -f pod名称  容器名称\n```\n\n","tags":["k8s"]},{"title":"基本概念","url":"/2022/09/27/k8s-concept/","content":"\n## master节点组件\n\n1.**kube-APIserver**： k8s的中心，各个组件信息交互都要通过它，也是k8s管理集群的入口。\n\n2.**Controller-Manager**：集群的状态管理器，保证Pod或其他资源达到期望值，也是需要和APIServer进行通信，在需要的时候创建、更新或删除它所管理的资源。\n\n3.**Scheduler**：k8s 集群的调度器，选择工作pod节点，预选 > 优选。\n\n4.**Etcd**：键值数据库，用于保存k8s集群信息，网络信息，网络信息。\n\n## node节点组件\n\n**kubelet**：Node节点管理，Pod管理，同容器运行时交互下发容器的创建/关闭命令，容器健康状态检查。\n\n**kube-proxy**：通过为Service资源的ClusterIP生成iptable或ipvs规则，实现将K8S内部的服务暴露到集群外面去。\n\n## 其他组件\n\ncalico：符合CNI标准的网络插件，给每个pod 生成一个唯一的IP地址，并且把每个节点当做一个路由器。\n\ncoreDns：用于kubernetes集群内部Service的解析，可以让Pod把Service名称解析成IP地址，然后通过Service的IP地址进行连接到对应的应用上。\n\n## pod概念\n\npod是kubernetes中最小的单元，它由一组、一个或多个容器1组成，每个pod还包含了一个pause容器，pause容器是pod的父容器，主要负责僵尸进程的回收管理，通过pause容器可以使同一个pod里面的多个容器共享存储、网络、pid、ipc等。\n\n## pod探针类型\n\nstartupProbe：k8s1.16版本后新增的功能，用于判断容器内应用程序是否已经启动，如果配置了startupProbe，就会先禁止其他的探针，直到它成功为止，成功后将不再进行探测。\n\nLivenessProbe:用于探测容器是否运行，如果探测失败，kubelet会根据配置的重启策略进行相应的处理。若没有配置该探针，默认就是success。\n\nReadinessProbe:一般用于探测容器内的程序是否健康，它的返回值如果为success，那么就代表这个容器已经完成启动，并且程序已经是可以接受流量的状态。\n\npod探针参数：\n\n initialDelaySeconds: 60  执行第一次探测前等待的时间为60秒\n\n periodSeconds: 10    检查间隔，每10秒进行一次探测\n\n successThreshold: 1  检查成功为1次表示就绪\n\n timeoutSeconds: 5   超时时间\n\n failureThreshold: 5  检查失败5次表示未就绪\n\n\n\n### 探针的检测方式\n\nExecAction：在容器内执行一个命令，如果返回值为0，则认为容器健康。\n\nTCPSocketAction：通过TCP连接检查容器内的端口是否是通的，如果是通的就认为容器健康。\n\nHTTPGetAction:通过应用程序暴露的API地址来检查程序是否是正常的，如果状态码为200~400之间，则认为容器健康。\n\n\n\n### startupProbe案例\n\n编写nginx.yaml\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-test\n  labels:\n    k8s-app: nginx\n    traffic-type: external\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-test\n  template:\n    metadata:\n      labels:\n        app: nginx-test\n    spec:\n      containers:\n      - name: nginx-test\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n        startupProbe:\n#          httpGet:\n#            path: /api/successStart\n#            port: 80\n          tcpSocket: \n            port: 80\n          failureThreshold: 1\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n#        readinessProbe:\n#          tcpSocket:\n#            port: 80\n#          failureThreshold: 1\n#          initialDelaySeconds: 10\n#          periodSeconds: 10\n#          successThreshold: 1\n#          timeoutSeconds: 2\n#        livenessProbe:\n#          tcpSocket:\n#            port: 80\n#          failureThreshold: 3\n#          initialDelaySeconds: 10\n#          periodSeconds: 10\n#          successThreshold: 1\n#          timeoutSeconds: 2\n```\n\n使用下面命令创建pod\n\n```shell\nkubectl create -f nginx.yaml\n```\n\n查看pod的状态，根据上面的配置将在10s左右显示成功。\n\n```shell\nkubectl get pod\n```\n\n![1664262320301](k8s-concept/1664262320301.png)\n\n使用下面的命令删除原来的pod\n\n```shell\nkubectl delete -f nginx.yaml\n```\n\n更改nginx.yaml探针检测的端口为81，然后重新创建pod\n\n```yaml\n        startupProbe:\n          tcpSocket: \n            port: 81\n          failureThreshold: 1\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n```\n\n查看pod的启动详情，由于没有启用这个端口而无法启动pod，报错如下截图\n\n```shell\nkubectl describe pod podname\n```\n\n![1670394166114](k8s-concept/1670394166114.png)\n\n\n\n### LivenessProbe 案例\n\n编辑livenessProbe.yaml\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-test\n  labels:\n    k8s-app: nginx\n    traffic-type: external\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-test\n  template:\n    metadata:\n      labels:\n        app: nginx-test\n    spec:\n      containers:\n      - name: nginx-test\n        image: nginx:1.7.9\n        args:\n        - /bin/sh\n        - -c\n        - touch /tmp/healthy; sleep 60; rm -f /tmp/healthy; sleep 600\n        envFrom:\n        - configMapRef:\n            name: game-config\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          exec:\n            command:\n            - cat \n            - /tmp/healthy\n          failureThreshold: 1\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n```\n\n创建如上资源清单的pod，探针每在10秒后进行检测，目录存在返回值为0，经过60秒后，探针再次检测发现目录被删除，返回值为非零，根据重启策略杀死容器并重新启动容器。\n\n![1670402448725](k8s-concept/1670402448725.png)\n\n并且重启的状态值加1\n\n![1670402515451](k8s-concept/1670402515451.png)\n\n### ReadinessProbe案例\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-test\n  labels:\n    k8s-app: nginx\n    traffic-type: external\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-test\n  template:\n    metadata:\n      labels:\n        app: nginx-test\n    spec:\n      containers:\n      - name: nginx-test\n        image: nginx:1.7.9\n        args:\n        - /bin/sh\n        - -c\n        - sleep 60;nginx -s stop\n        envFrom:\n        - configMapRef:\n            name: game-config\n        ports:\n        - containerPort: 80\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          failureThreshold: 1\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n```\n\n该案例是检测容器内部的应用是否健康，探针每10秒进行健康检查，一分钟后停止容器内的应用，则在70分钟左右容器会进行重启。\n\n## 其他问题：\n\n探针的健康检查是针对pod启动前的检查，关于停止退出后，还有些请求未处理完，这个问题我们该如何去判断它处理完，然后进行安全的停止的呢？答案是preStop做相关配置\n\n\n\n当我们执行delete pod操作时，pod状态有哪些变化：\n\n1.pod状态由running变成Terminating\n\n2.Endpoint删除该pod的IP地址\n\n3.执行PreStop的指令\n\n\n\nPreStop:先去请求eureka接口，把自己的ip地址和端口号，进行线下，eureka从注册表中删除该应用的ip地址。\n\n\n\nterminationGracePeriodSeconds: 30这个参数是k8s预留时间让容器处理关闭还在执行的操作，如果配置了sleep: 5 ,进程5秒内没办法关闭，则会等待30秒，此时sleep就失效了。\n\n\n\n\n\n","tags":["k8s"]},{"title":"Dockerfile","url":"/2022/09/27/dockerFile/","content":"\ndockerfile指令\n\n```\nFROM 构建镜像基于哪个镜像\nMAINTAINER 镜像维护者姓名或邮箱地址\nRUN 构建镜像时运行的指令\nCMD 运行容器时执行的shell环境\nVOLUME 指定容器挂载点到宿主机自动生成的目录或其他容器\nUSER 为RUN、CMD、和 ENTRYPOINT 执行命令指定运行用户\nWORKDIR 为 RUN、CMD、ENTRYPOINT、COPY 和 ADD 设置工作目录，就是切换目录\nHEALTHCHECH 健康检查\nARG 构建时指定的一些参数\nEXPOSE 声明容器的服务端口（仅仅是声明）\nENV 设置容器环境变量\nADD 拷贝文件或目录到容器中，如果是URL或压缩包便会自动下载或自动解压\nCOPY 拷贝文件或目录到容器中，跟ADD类似，但不具备自动下载或解压的功能\nENTRYPOINT 运行容器时执行的shell命令\n```\n\n注意：\n\nCMD和ENTRYPOINT必须有一个\n\nCMD可以被覆盖，如果有ENTRYPIOINT的话，CMD就是ENTRYPIOINT的参数\n\n简单的例子\n\n```shell\nFROM centos:7\nLABEL maintainer=\"test dockerfile\"\nLABEL test=dokerfile\nRUN useradd dockerfile\nRUN mkdir /opt/dot\nCMD [\"sh\",\"-c\",\"echo 1\"]\n```\n\n使用下面命令创建镜像\n\n```shell\ndocker build -t mycentos:8 .\n#运行镜像\ndocker run -it --rm mycentos:8 bash\n```\n\n\n\n制作镜像原则：\n\n一定不要使用centos为基础镜像，建议使用alpine\n\n编译程序和构建镜像要分开","tags":["docker"]},{"title":"k8s安装（centos7）","url":"/2022/09/03/k8s-install/","content":"\n提示：如果你还没有接触过linux，一些简单的指令还未学会使用，建议先学习一下一些linux入门的知识。\n\n\n\n### 基础环境配置\n\n环境：三台master,两台node,配置：40G硬盘以上 2核4G内存，ip地址和主机名如下：\n\n```shell\n192.168.1.11 master01\n192.168.1.12 master02\n192.168.1.13 master03\n192.168.1.14 node01\n192.168.1.15 node02\n```\n\n1.配置所有机器的hosts文件\n\n```shell\nvim /etc/hosts\n192.168.1.11 master01\n192.168.1.12 master02\n192.168.1.13 master03\n192.168.1.14 node01\n192.168.1.15 node02\n```\n\n2.所有机器关闭防火墙，dnsmasq,selinux,swap\n\n```shell\n#关闭防火墙\nsystemctl disable --now firewalld\n#关闭dns\nsystemctl disable dnsmasq\n#设置selinux为disabled\nvim /etc/selinux/config\n#设置swap\nswapoff -a && sysctl -w vm.swappiness=0\n#关闭setenforce\nsetenforce 0\n```\n\n3.vim /etc/fstab所有机器注释掉最后一行，关闭交换分区\n\n```shell\n# Created by anaconda on Wed Dec  8 21:55:51 2021\n#\n# Accessible filesystems, by reference, are maintained under '/dev/disk'\n# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n#\n/dev/mapper/centos-root /                       xfs     defaults        0 0\nUUID=735ad1c5-2c28-41b5-9131-d2413f740937 /boot                   xfs     defaults        0 0\n#/dev/mapper/centos-swap swap                    swap    defaults        0 0 #注释改行\n```\n\n4.安装ntpdate实现时间同步\n\n```shell\n#查看是否已经安装ntp\nrpm -qa | grep ntp\n#安装ntp\nyum install -y ntp\n#所有节点做以下配置\nln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\necho 'Asia/Shanghai' > /etc/timezone\nntpdate time2.aliyun.com\n#所有节点添加定时任务\n*/5 * * * * ntpdate time2.aliyun.com\n```\n\n5.所有节点配置连接数最大值\n\n```\nulimit -SHn 65535\n```\n\n6.配置免密使得master01可以访问其他节点，这步比较简单就不做详述，可参考免密登录配置\n\n7.所有节点安装常用的工具\n\n```shell\nyum install -y yum-utils device-mapper-persistent-data lvm2 net-tools\n```\n\n8.所有节点升级系统（该步骤还没有升级内核）\n\n```shell\nyum update -y --exclude=kernel* && reboot\n```\n\n9.所有节点内核升级到4.18以上，centos8就不用了\n\n```\n#在 CentOS 7 上启用 ELRepo 仓库\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm\n#仓库启用后，你可以使用下面的命令列出可用的内核相关包\nyum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available\n#安装最新的主线稳定内核\nyum --enablerepo=elrepo-kernel install kernel-ml -y \n#设置 GRUB 默认的内核版本\ngrub2-set-default 0\n#重启\nreboot\n#查看版本\nuname -r\n```\n\n10.所有节点安装ipvsadm\n\n```shell\nyum install ipvsadm ipset sysstat conntrack libseccomp -y\n```\n\n所有节点配置ipvs模块，在内核4.19版本nf_conntrack_ipv4已经改为nf_conntrack，本例\n\n安装的内核为4.18，使用nf_conntrack_ipv4\n\n```shell\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\n\nvim /etc/modules-load.d/ipvs.conf\nip_vs\nip_vs_rr\nip_vs_wrr\nip_vs_sh\nnf_conntrack_ipv4\nip_tables\nip_set\nxt_set\nipt_set\nipt_rpfilter\nipt_REJECT\nipip\n\n执行systemctl enable --now systemd-modules-load.service设置开机自启动\n检查是否已经加载lsmod | grep -e ip_vs -e nf_conntrack_ipv4,如下图说明成功\n```\n\n![1663508523660](k8s-install/1663508523660.png)\n\n### 镜像源的配置\n\n11.使用阿里云yum源，先备份原来的源。\n\n```shell\ncp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.back\ncurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n\n#添加阿里云docker镜像仓库\nyum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n```\n\n12.创建k8s源，如下图所示\n\n```shell\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\n#删除匹配到的字符串\nsed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo\n```\n\n![1670307130989](/k8s-install/1670307130989.png)\n\n### 安装配置docker\n\n13所有节点安装docker，可以参考本人的docker安装教程文章\n\n### 安装kubeadm\n\n14.所有节点安装kubeadm\n\n```shell\n#查看kubeadm版本\nyum list kubeadm.x86_64 --showduplicates | sort -r\n yum install kubeadm-1.22.0-0.x86_64 kubectl-1.22.0-0.x86_64 kubelet-1.22.0-0.x86_64  -y\n```\n\n15.默认配置的pause镜像使用gcr.io仓库，国内可能无法访问，所以这里配置kubelet使用阿里云的pause镜像\n\n```\ncat >/etc/sysconfig/kubelet<<EOF\nKUBELET_EXTRA_ARGS=\"--cgroup-driver=systemd --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2\"\nEOF\n#启动kubelet\nsystemctl daemon-reload\nsystemctl enable --now kubelet\n```\n\n### 安装高可用组件\n\n16.安装高可用组件，只需要在master上安装即可\n\n```shell\nyum -y install keepalived haproxy\n\n#配置haproxy\nvim /etc/haproxy/haproxy.cfg\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local2 err\n\n    maxconn     4000\n    ulimit-n 16384\n    stata timeout 30s\n    # turn on stats unix socket\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    timeout http-request    15s\n    timeout connect         500s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 15s\nfrontend monitor-in\n    bind *:33305\n    mode http\n    option httplog\n    monitor-uri /monitor\nlisten stats\n    bind *:8006\n    mode http\n    stats enable\n    stats hide-version\n    stats uri /stats\n    stats refresh 30s\n    stats realm Haproxy\\ Statistics\n    stats auth admin:admin\nfrontend master\n    bind 0.0.0.0:16443\n    bind 127.0.0.1:16443\n    mode tcp\n    option tcplog\n    tcp-request inspect-delay 5s\n    default_backend master\n\nbackend master\n    mode tcp\n    option tcplog\n    option tcp-check\n    balance roundrobin\n    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxgueue 256 weight 100\n\n    server master01 192.168.1.11:6443 check\n    server master02 192.168.1.12:6443 check\n    server master03 192.168.1.13:6443 check\n\n\n#配置keepalived\n#master01\nglobal_defs {\n     router_id master01             #标识信息，一个名字而已\n  }  \nvrrp_script chk_apiserver {\n\t script \"/etc/keepalived/check.sh\"\n\t interval 2\n\t weight -5\n\t fall 3\n\t rise 2\n}\n  vrrp_instance VI_1 {\n      state MASTER\t\t        #角色是master\n      interface ens33           #vip绑定端口\n      mcast_src_ip 192.168.1.11\n      virtual_router_id 51      #让master和backup在同一个虚拟路由中，id号必须一样\n      priority 95               #优先级，谁高，谁就是master\n      advert_int 1              #心跳间隔时间\n      authentication {\n          auth_type PASS        #认证\n          auth_pass 1111        #密码\n      }\n      virtual_ipaddress {\n          192.168.1.200        #虚拟ip\n      }\n      track_script {\n\t      chk_apiserver\n      }\n  }\n \n \n #master02配置文件\n ! Configuration File for keepalived\n \n  global_defs {\n     router_id master02             #标识信息，一个名字而已\n  }  \n  vrrp_script chk_apiserver {\n\t script \"/etc/keepalived/check.sh\"\n\t interval 2\n\t weight -5\n\t fall 3\n\t rise 2\n}\n  vrrp_instance VI_1 {\n      state BACKUP\t\t        #角色是backup\n      interface ens33           #vip绑定端口\n      mcast_src_ip 192.168.1.12\n      virtual_router_id 51      #让master和backup在同一个虚拟路由中，id号必须一样\n      priority 91               #优先级，谁高，谁就是master\n      advert_int 1              #心跳间隔时间\n      authentication {\n          auth_type PASS        #认证\n          auth_pass 1111        #密码\n      }\n      virtual_ipaddress {\n          192.168.1.200        #虚拟ip\n      }\n      track_script {\n\t      chk_apiserver\n      }\n  }\n  \n#master03同理\nglobal_defs {\n     router_id master03             #标识信息，一个名字而已\n  }  \n  vrrp_script chk_apiserver {\n\t script \"/etc/keepalived/check.sh\"\n\t interval 2\n\t weight -5\n\t fall 3\n\t rise 2\n}\n  vrrp_instance VI_1 {\n      state BACKUP\t\t        #角色是backup\n      interface ens192           #vip绑定端口\n      mcast_src_ip 192.168.1.13\n      virtual_router_id 51      #让master和backup在同一个虚拟路由中，id号必须一样\n      priority 80               #优先级，谁高，谁就是master\n      advert_int 1              #心跳间隔时间\n      authentication {\n          auth_type PASS        #认证\n          auth_pass 1111        #密码\n      }\n      virtual_ipaddress {\n          192.168.1.200        #虚拟ip\n      }\n      track_script {\n\t      chk_apiserver\n      }\n  }\n\n```\n\n添加健康脚本\n\n```shell\n#!/bin/sh\n#Author:hw                  \nnginxpid=$(ps -C nginx --no-header|wc -l)\nif [ $nginxpid -eq 0 ]\n        then\n         /usr/local/nginx2/sbin/nginx\n         echo \"finish\" > /opt/nginx.log\n         sleep 3\n         nginxpid=$(ps -C nginx --no-header|wc -l)\n         if [ $nginxpid -eq 0 ]\n         then\n          systemctl stop keepalived\n         fi\nfi\n```\n\n\n\n```shell\n#启动haproxy和keepalived\nsystemctl start haproxy\nsystemctl start keepalived\n\nsystemctl enable haproxy\nsystemctl enable keepalived\n```\n\n\n\n### kubeadm初始化\n\n17kubeadm集群初始化，只需在master节点上操作\n\n使用面命令生成配置文件\n\n```\n kubeadm config print init-defaults >kubeadm-config.yaml\n```\n\n编辑vim kubeadm-config.yaml(减少初始化时间)\n\n```\napiVersion: kubeadm.k8s.io/v1beta2\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 192.168.1.11\n  bindPort: 6443\nnodeRegistration:\n  criSocket: /var/run/dockershim.sock\n  name: master01\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\n---\napiServer:\n  timeoutForControlPlane: 4m0s\napiVersion: kubeadm.k8s.io/v1beta2\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrollerManager: {}\ndns:\n  type: CoreDNS\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\nkind: ClusterConfiguration\nkubernetesVersion: v1.24.0 #改成自己的版本\nnetworking:\n  dnsDomain: cluster.local\n  podSubnet: \"10.244.0.0/16\"\n  serviceSubnet: 10.96.0.0/12\nscheduler: {}\n---\napiVersion: kubeprocy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nfreatureGates:\n  SupportIPVSProxyMode: true\nmode: ipvs\n```\n\n#如果配置文件的版本过低可以使用下面命令更新\n\n```\nkubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml\n```\n\n#通过配置文件拉取相关的镜像\n\n```shell\nkubeadm config images pull --config /root/kubeadm-config.yaml\n```\n\n#初始化命令\n\n```\nkubeadm init --config /root/kubeadm-config.yaml --upload-certs\n```\n\n#或者使用参数的形式初始化\n\n```\nkubeadm init --apiserver-advertise-address=192.168.1.11 --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers --kubernetes-version v1.22.0 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.50.0.0/16\n```\n\n#初始化失败时，使用下面命令清空\n\n```shell\nkubeadm reset\n```\n\n#初始化完成后要记下图中几行\n\n![1663764621945](k8s-install/1663764621945.png)\n\n#相关报错，如下图\n\n![1663729431385](k8s-install/1663729431385.png)\n\n处理方法\n\n```\nrm /etc/containerd/config.toml\nsystemctl restart containerd\n```\n\n### 安装calico网络插件\n\n18.calico组件安装\n\n访问calico官网https://projectcalico.docs.tigera.io/\n\n![1663654833662](k8s-install/1663654833662.png)\n\nmaster01节点执行下面语句，下载文件\n\n```\ncurl https://docs.projectcalico.org/manifests/calico.yaml -O\n```\n\n#可点击下图此处查看安装的calico版本，适应的k8s版本\n\n![1663656550612](k8s-install/1663656550612.png)\n\n#根据k8s配置文件的网段更改calicao.yaml的配置文件，和pod-network-cidr的地址一样\n\n![1663766204521](k8s-install/1663766204521.png)\n\n#创建calico\n\n```\nkubectl apply -f calico.yaml\n```\n\n#查看master节点状态\n\n```\nkubectl get po -n kube-system\n```\n\n### node加入master\n\n19.使用token将node加入master节点，初始化生成的token指令\n\n```shell\n#使用该命令将node节点加入\nkubeadm join 192.168.1.11:6443 --token sthh0o.n0br43dscp2znn58 \\\n\t--discovery-token-ca-cert-hash sha256:70152c9dea452648c7dcd08d319be54077a1cf4e7f77787fade38fb2b098736a\n#使用该命令将master节点加入\nkubeadm join 192.168.1.11:6443 --token sthh0o.n0br43dscp2znn58 \\\n\t--discovery-token-ca-cert-hash sha256:70152c9dea452648c7dcd08d319be54077a1cf4e7f77787fade38fb2b098736a \\\n\t--control-plane  --certificate-key c5c0cafca32b27454763f5cdd59b7c464c28ef4b0043dd13b56b3328c49b94e9 #由下面命令生成\n#获取主节点的join命令\n kubeadm token create --print-join-command\n#使用该命令重新生成token\nkubeadm init phase upload-certs --upload-certs\n#查看加入的情况\nkubectl get node\n#修改相关配置\nkubectl -n kube-system edit cm kubeadm-config\n```\n\n到这里就完成k8s的安装，下面是k8s监控组件的安装，不是必须的。\n\n\n\n\n\n### 安装metrics\n\n20.Metrics监控pod和node的cpu和memory指标\n\n```shell\n#修改/etc/kubernetes/manifests/kube-apiserver.yaml配置文件，添加enable-aggregator-routing\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --advertise-address=192.168.200.3\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction\n    - --enable-bootstrap-token-auth=true\n    - --enable-aggregator-routing=true            # 添加本行\n\n#下载yaml文件\nwget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml\n#修改components.yaml文件\nspec:\n      containers:\n      - args:\n        - --cert-dir=/tmp\n        - --secure-port=4443\n        - --kubelet-preferred-address-types=InternalIP   # 删掉 ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --kubelet-insecure-tls           #   加上该启动参数\n        image: registry.cn-hangzhou.aliyuncs.com/chenby/metrics-server:v0.4.1   # 镜像地址使用阿里云\n        \n        \n#执行该命令安装\nkubectl apply -f components.yaml\n\n#查看服务状态，Running则正常 \nkubectl get pod -n kube-system | grep metrics-server\n#检查 API Server 是否可以连通 Metrics Server\nkubectl describe svc metrics-server -n kube-system\n\nName:              metrics-server\nNamespace:         kube-system\nLabels:            k8s-app=metrics-server\nAnnotations:       <none>\nSelector:          k8s-app=metrics-server\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.1.178.198\nIPs:               10.1.178.198\nPort:              https  443/TCP\nTargetPort:        https/TCP\nEndpoints:         10.50.196.133:4443\nSession Affinity:  None\nEvents:            <none>\n\n#在各个节点上ping 10.50.196.133\n#查看节点占用性能情况\n kubectl top nodes\n```\n\n### 安装dashbord\n\n21.安装dashboard\n\n```\n#下载配置文件，如果无法下载，可以手动创建\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n#安装\nkubectl apply -f recommended.yaml\n#执行kubectl edit svc kubernetes-dashboard，修改type值为NodePort\ntype: NodePort\n\n#创建用户，编辑文件vim admin.yaml，加入下面配置，然后执行kubectl apply -f admin.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kube-system\n#查看token\nkubectl -n kube-system describe secret $(kebectl -n kube-system get secret | grep admin-user | awk '{print $1}')\n```\n\n","tags":["k8s"]},{"title":"docker网络","url":"/2022/08/18/dockerNetwork/","content":"\n\n\n#### 为什么\n\n1.容器间的互联和通信及端口映射。\n\n2.当容器ip变动时可以通过服务名进行连接而不受影响。\n\n\n\n#### 网络模式\n\n\n\n| 类型      | 说明                                                         |\n| --------- | :----------------------------------------------------------- |\n| Host      | 容器不会虚拟出自己的网卡，配置主机的IP等，而是使用宿主机的IP和端口 |\n| None      | 该模式关闭了容器的网络功能                                   |\n| Container | 创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP，端口范围 |\n| Bridge    | 默认为该模式，此模式会为每一个容器分配，设置IP等，并将容器连接到一个docker0 的虚拟网桥，通过docker0 网桥以及iptables nat 表配置与宿主机通信 |\n\n\n\n使用方式在docker命令中添加参数--network host即可指定网络模式\n\n\n\nbridge模式：\n\n![image01](/dockerNetwork/image01.png)\n\n\n\nhost模式\n\n一句话总结：就是没有自己的虚拟网卡，ip地址，直接使用宿主机的网络\n\n\n\n\n\n#### 常用命令\n\n```\ndocker network ls \ndocker network --help\ndocker network create 网络名\ndocker network rm 网络名\n```\n\n如果在创建网络的时候出现下面的报错，重启一下docker即可\n\n```\nError response from daemon: Failed to Setup IP tables: Unable to enable SKIP DNAT rule:  (iptables failed: iptables --wait -t nat -I DOCKER -i br-182add84403e -j RETURN: iptables: No chain/target/match by that name.\n```\n\n","tags":["docker"]},{"title":"docker数据的存储","url":"/2022/08/03/dockerStorage/","content":"\n## 简介\n\n数据的存储主要分为持久化和非持久化，相对于docker而言，非持久化的数据会随着容器的销毁而被删除，而持久化的数据则不会。由于在生产环境中为了保证数据的安全，不被丢失，我们都会使数据持久化，接下来重点说一下如何进行持久化。\n\n## 持久化\n\n你可以创建数据卷，然后创建容器，接着将卷挂载到容器上。卷会挂载到容器文件系统的某个目录之下，任何写到该目录下的内容都会写到卷中。即使容器被删除，卷与其上面的数据仍然存在。\n\n\n\n​\t命令格式：\n\n```\ndocker run -d --privileged=true -v /宿主机绝对路径:/容器内目录 镜像名\n```\n\n在主机创建或者对文件内容数据的更改，容器内的目录也会有同步到，反之亦然。也就是说数据是双向同步的。\n\n```\ndocker run -d --privileged=true -v /宿主机绝对路径:/容器内目录:rw 镜像名\ndocker run -d --privileged=true -v /宿主机绝对路径:/容器内目录:ro 镜像名\n```\n\n当我们在容器的目录后面不添加任何读写权限的时候，默认是rw，如上面第一条命令。如果我们限制容器只读的时候后面添加ro，如上面第二条指令。\n\n\n\n​\t卷的继承和共享：\n\n```\ndocker run -it --privileged=true --volumes-from 父类的容器名 --name 容器名称 镜像\n```\n\n\n\n## 其他数据卷的指令：\n\n\n\n​\t使用如下命令创建新数据卷：\n\n```shell\ndocker volume create myvol\n```\n\n​\t使用下面命令查看本地使用的是什么类型的存储驱动\n\n```\ndocker info\n```\n\n​\t使用下面命令查看所创建的数据卷信息\n\n```shell\ndocker volume ls \ndocker volume inspect volumeName #查看详细信息\ndocker volume ls -f dangling=ture #查看没在使用的数据卷\n```\n\n​\t使用下面命令删除所创建的卷，都不能删除容器正在使用的卷\n\n```\ndocker volume prune #删除全部，不建议使用\ndocker volume rm xxx #允许删除指定的卷\n```\n\n\n\n## 注意事项：\n\n![1659668394717](/dockerStorage/1659668394717.png)\n\n","tags":["docker"]},{"title":"docker简单应用，和指令","url":"/2022/07/27/dockerCommand/","content":"\n## 镜像的基本命令\n\n### 帮助命令\n\n```shell\ndocker version\t\t#显示docker的版本信息\ndocker info\t\t\t#显示docker的系统信息\ndocker --help\t\t#显示docker的帮助信息\n```\n\n### 镜像命令\n\ndocker images [可选项] 查看所有本地的主机上的镜像\n\n```shell\ndocker images\n#解释\nREPOSITORY 镜像的仓库源\nTAG \t   镜像的标签\nIMAGE ID   镜像的id\nCREATED\t   镜像的创建时间\nSIZE       镜像的大小\n\n#可选项\n-a --all\t#列出所有镜像\n-q --quiet\t#只显示镜像id\n```\n\ndocker search imageName 搜索镜像\n\n```\ndocker search\n#可选项\n--filter=STARS=3000\t#搜索镜像STARS大于3000的\n```\n\ndocker pull 下载镜像\n\n```shell\n#下载镜像docker pull 镜像名[:tag]\ndocker pull mysql\n#指定版本下载\ndocker pull mysql:5.7\n#推送镜像到远程\ndocker push hw.harbor.com/\n```\n\n删除镜像\n\n```shell\ndocker rmi -f #删除镜像\n\ndocker rmi -f $(docker images -aq) #删除全部的容器\n```\n\n```\n#查看镜像的历史构建\ndocker history imageId\n```\n\n将镜像ubuntu:15.10标记为 runoob/ubuntu:v3 镜像。\n\n```\ndocker tag ubuntu:15.10 runoob/ubuntu:v3\n```\n\n\n\n### 容器指令\n\n```shell\ndocker run [可选参数] image\n#参数说明\n--name=\"Name\"\t容器名字\n-d\t\t\t\t后台方式运行\n-it\t\t\t\t使用交互方式运行，进入容器查看内容\n-p\t\t\t\t指定容器的端口 -p 8080:8080\n-p\t\t\t\t随机指定端口\n```\n\n列出所有运行的容器\n\ndocker ps 命令\n\n```shell\ndocker ps #列出当前正在运行的容器\n#选项\n-a \t#列出当前运行的容器和历史运行过的容器\n-n=?\t#显示最近创建的容器\n-q\t#只显示容器的编号\n```\n\n退出容器\n\n```shell\nexit\t#直接容器停止退出\nCtrl+P+Q\t#容器不停止退出\n```\n\n删除容器\n\n```shell\ndocker rm 容器id + -f 强制删除\ndocker rm -f $(docker ps -aq) #删除所有的容器\n```\n\n启动停止容器\n\n```shell\ndocker restart 容器id\ndocker start 容器id \ndocker stop 容器id\n```\n\n### 其他命令\n\n后台启动容器\n\n```shell\ndocker run -d 镜像名！\n```\n\n查看日志\n\n```shell\ndocker logs -tf  --tail 10 容器\n```\n\n查看容器的进程信息\n\n```\ndocker top id\n```\n\n查看镜像的元数据\n\n```shell\ndocker inspect id\n```\n\n进入当前的运行容器\n\n```shell\ndocker exec -it id /bin/bash\t#进入容器后一个新的终端\ndocker attach id\t#进入当前正在启动的终端\n```\n\n从容器拷文件到服务器上\n\n```shell\ndocker cp id:/home/test.java /home\n```\n\n查看日志，相关格式如下图\n\n![1658828268930](/dockerCommand/1658828268930.png)\n\n```shell\ndocker logs --since 30m 容器id #查看最近30分钟的日志\ndocker logs -f 容器id #实时查看日志\n```\n\n\n\n拷贝文件\n\n```\ndocker cp\n```\n\n\n\n### 案例：docker运行mysql\n\n依次使用下面的指令将mysql运行起来\n\n```\ndocker search mysql\t#搜索镜像\ndocker pull mysql:8.0 #获取镜像\ndocker run -d -p 3306:3306  -e MYSQL_ROOT_PASSWORD=102850 --name mysql01 -v /opt/docker/mysql/conf/my.cnf:/etc/mysql/my.cnf -v /opt/docker/mysql/data:/var/lib/mysql mysql:8.0\n\t#运行镜像\ndocker ps\t#显示正在运行的镜像\n```\n\n目标：到官方仓库https://hub.docker.com/选择对应版本的mysql，拉取镜像并运行。\n\n![1658821554668](/dockerCommand/1658821554668.png)\n\n升级mysql版本\n\n先从网上拉取要升级的版本的mysql，我这里从8.0-->8.0.29\n\n![1658915175012](/dockerCommand/1658915175012.png)\n\n先停掉低版本的mysql\n\n![1658915333794](/dockerCommand/1658915333794.png)\n\n为了安全起见，最好先做好备份\n\n```shell\ntar -zcvf mysql_back.tar.gz /opt/docker/mysql\n```\n\n启动高版本的镜像，并指定数据路径\n\n```\ndocker run --name=mysql02 \\\n   -p 3320:3306 \\\n   --privileged=true \\\n   --mount type=bind,src=/opt/docker/mysql/conf/my.cnf,dst=/etc/mysql/conf.d \\\n   --mount type=bind,src=/opt/docker/mysql/data,dst=/var/lib/mysql \\\n   --restart=always \\\n   -e MYSQL_ROOT_PASSWORD=123456 \\\n   -d mysql:8.0.29\n```\n\n","tags":["docker"]},{"title":"docker安装(centos7)","url":"/2022/07/16/docker-install/","content":"\n第一步：查看linux内核是否符合安装要求，官方要求内核大于3.10。\n\n```\n[root@web09 ~]# uname -r\n3.10.0-1160.59.1.el7.x86_64\n```\n\n第二步：删除原来的docker，如果没有可以跳过\n\n```\nyum remove docker  docker-common docker-selinux docker-engine\n```\n\n第三步：安装相关的依赖包\n\n```\nyum install -y yum-utils device-mapper-persistent-data lvm2\n```\n\n![1657945029971](docker-install/1657945029971.png)\n\n第四步：配置yum源\n\n```shell\n[root@localhost ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n已加载插件：fastestmirror\nadding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\ngrabbing file http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo\nrepo saved to /etc/yum.repos.d/docker-ce.repo\n```\n\n第五步：查看docker的版本，并选择要安装的版本进行安装\n\n```shell\nyum list docker-ce --showduplicates | sort -r\n```\n\n![1658751430885](docker-install/1658751430885.png)\n\n第六步：选择要安装的版本\n\n```shell\nyum -y install docker-ce-20.10.8-3.el7\n```\n\n第七步：设置镜像加速器\n\n```shell\ncat > /etc/docker/daemon.json <<EOF\n{\n\"exec-opts\":[\"native.cgroupdriver=systemd\"],\n\"registry-mirrors\": [\"http://hub-mirror.c.163.com\"]\n}\nEOF\n```\n\n第八步：重启并设置开机自启动\n\n```shell\nsystemctl start docker\nsystemctl enable docker\n```\n\n","tags":["docker"]},{"title":"ftp安装配置（虚拟用户）","url":"/2022/07/07/ftpInstall/","content":"\n1.安装vsftp和ftp\n\n```shell\nyum -y install vsftp\nyum -y install ftp\n```\n\n2.创建虚拟用户对应的本地用户和目录\n\n```shell\nuseradd ftp -s /sbin/nologin\nmkdir -p /var/ftp/vuser1 /var/ftp/vuser2\nchown -R ftp:ftp /var/ftp/vuser1 /var/ftp/vuser2\nchmod -R 755 /var/ftp/vuser1 /var/ftp/vuser2\n```\n\n3.进入ftp配置文件目录（/etc/vsftpd/）创建相关文件夹和文件\n\n```\nmkdir /etc/vsftpd/vconf #存放相关的虚拟用户配置文件\n```\n\n```shell\nvim /etc/vsftpd/vuser.list #新建文件存放虚拟用户的账号密码，第一行为账号，第二行为密码\nuser1\n123456\nuser2\n123456\n```\n\n4.生成vuser.db文件，并赋权限为600\n\n```shell\ndb_load -T -t hash -f /etc/vsftpd/vuser.list /etc/vsftpd/vuser.db\nchmod 600 /etc/vsftpd/vuser.db\n```\n\n5.创建相关的PAM文件，所在目录为/etc/pam.d/\n\n```shell\n#先备份原来的文件\ncp /etc/pam.d/vsftpd /etc/pam.d/vsftpd.bak\n\n#删除里面所以内容，加入下面两句\nauth required /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser\naccount required /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser\n```\n\n6.编写主配置文件/etc/vsftpd/vsftpd.conf，内容如下\n\n```shell\n#不允许匿名用户登录\nanonymous_enable=NO\n#不允许匿名用户上传文件\nanon_upload_enable=NO\n#不允许匿名用户创建目录\nanon_mkdir_write_enable=NO\n#允许使用系统用户访问\nlocal_enable=YES\n#允许本地用户读写\nwrite_enable=YES\n#设置匿名用户的umask值\nlocal_umask=022\ndirmessage_enable=YES\nxferlog_enable=YES\nconnect_from_port_20=YES\nchown_uploads=NO\nxferlog_file=/var/log/vsftpd.log\nxferlog_std_format=YES\nasync_abor_enable=YES\nascii_upload_enable=YES\nascii_download_enable=YES\nftpd_banner=Welcome to JoiWay-FTP Server\nchroot_local_user=YES\nallow_writeable_chroot=YES\nls_recurse_enable=NO\nlisten=YES\nhide_ids=YES\n#改成自己定义的文件\npam_service_name=vsftpd\nuserlist_enable=YES\ntcp_wrappers=YES\nguest_enable=YES\n#虚拟用户对应的本地用户\nguest_username=ftp\nvirtual_use_local_privs=YES\n#改成自己创建的目录\nuser_config_dir=/etc/vsftpd/vconf\nlisten_address=192.168.0.*\nlisten_port=21\npasv_enable=YES\npasv_min_port=40000\npasv_max_port=40080\npasv_promiscuous=YES\n```\n\n7.创建虚拟用户对应的配置文件\n\n```shell\nvim /etc/vsftpd/vconf/user1 #添加如下内容\n\n#指定虚拟用户的具体主路径\nlocal_root=/var/ftp/user1\n#设定不允许匿名用户访问\nanonymous_enable=NO\n#设定不允许匿名用户上传\nanon_upload_enable=NO\n#设定不允许匿名用户建立目录\nanon_mkdir_write_enable=NO\n#设定允许写操作\nwrite_enable=YES\n#设定上传文件权限掩码\nlocal_umask=022\n#设定空闲连接超时时间\nidle_session_timeout=600\n#设定单次连续传输最大时间\ndata_connection_timeout=120\n#设定并发客户端访问个数\nmax_clients=10\n#设定单个客户端的最大线程数，这个配置主要来照顾Flashget、迅雷等多线程下载软件\nmax_per_ip=5\n#设定该用户的最大传输速率，单位b/s\n#local_max_rate=50000\n\n\nvim /etc/vsftpd/vconf/user2\n\n#指定虚拟用户的具体主路径\nlocal_root=/var/ftp/user2\n#设定不允许匿名用户访问\nanonymous_enable=NO\n#设定不允许匿名用户上传\nanon_upload_enable=NO\n#设定不允许匿名用户建立目录\nanon_mkdir_write_enable=NO\n#设定允许写操作\nwrite_enable=YES\n#设定上传文件权限掩码\nlocal_umask=022\n#设定空闲连接超时时间\nidle_session_timeout=600\n#设定单次连续传输最大时间\ndata_connection_timeout=120\n#设定并发客户端访问个数\nmax_clients=10\n#设定单个客户端的最大线程数，这个配置主要来照顾Flashget、迅雷等多线程下载软件\nmax_per_ip=5\n#设定该用户的最大传输速率，单位b/s\n#local_max_rate=50000\n```\n\n8.最后重启vsftp，用windows测试连接即可\n\n```shell\nsystemctl restart vsftpd\n```\n\n![1657113184013](/ftpInstall/1657113184013.png)","tags":["linux"]},{"title":"gitlab更新","url":"/2022/07/06/gitlabUpdate/","content":"\n根据官方的更新规则，如需要更新必须从下往上一级一级的更新，更新前先到镜像网址下载相关版本的安装包，官网版本的更新顺序如下。\n\n```\n8.11.Z -> 8.12.0 -> 8.17.7 -> 9.5.10 -> 10.8.7 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.10.14 -> 13.0.14 -> 13.1.11 -> 13.8.8 -> 13.12.15 -> 14.0.12 -> 14.3.6 -> 14.9.5 -> 14.10.Z -> 15.0.Z -> latest 15.Y.Z\n```\n\n\n\n例如当前我的版本为10.8.3版本，如果要更新至最新版本14.10.5，则更新的顺序为10.8.7 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.10.14 -> 13.0.14 -> 13.1.11 -> 13.8.8 -> 13.12.15 -> 14.0.12 -> 14.3.6 -> 14.9.5 -> 14.10.5。到镜像网址下载相关rpm包https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/，所需的包如下，我这里使用的是社区版\n\n![1657011075104](/gitlabUpdate/1657011075104.png)\n\n\n\n接下来开始更新\n\n1.备份。默认的备份路径为：/var/opt/gitlab/backups，自定义的备份路径在配置文件/etc/gitlab/gitlab.rb中查看，备份会生成1656923978_2022_07_04_10.8.3_gitlab_backup.tar格式的压缩包\n\n```\ngitlab-rake gitlab:backup:create\n```\n\n![1657011728436](/gitlabUpdate/1657011728436.png)\n\n如果需要还原用如下命令\n\n```\ngitlab-rake gitlab:backup:restore BACKUP=1656923978_2022_07_04_10.8.3\n```\n\n2.停止三个gitlab子服务，注意不要全部停止\n\n```\ngitlab-ctl stop unicorn\ngitlab-ctl stop puma\ngitlab-ctl stop sidekiq\n```\n\n3.使用yum命令更新，每更新一个版本最好都要重新加载配置和重启。然后再重复第二、三的步骤\n\n```\nyum install -y --setopt=obsoletes=0 gitlab-ce-10.8.7-ce.0.el7.x86_64.rpm\n#重新加载配置\ngitlab-ctl reconfigure\ngitlab-ctl restart\n\nyum install -y --setopt=obsoletes=0 gitlab-ce-11.11.8-ce.0.el7.x86_64.rpm\n#重新加载配置\ngitlab-ctl reconfigure\ngitlab-ctl restart\n\nyum install -y --setopt=obsoletes=0 gitlab-ce-12.0.12-ce.0.el7.x86_64.rpm\n#重新加载配置\ngitlab-ctl reconfigure\ngitlab-ctl restart\n\n.\n.\n.\n.\n.\n.\n```\n\n4.最后用cat /opt/gitlab/embedded/service/gitlab-rails/VERSION命令查看版本。\n\n![1657032763986](/gitlabUpdate/1657032763986.png)\n\n\n\n5.报错，可能会有下面的报错信息，跟着提示一条一条的执行gitlab-rake gitlab:background_migrations:finalize...........即可\n\n```\nThere was an error running gitlab-ctl reconfigure:\n\nrails_migration[gitlab-rails] (gitlab::database_migrations line 51) had an error: Mixlib::ShellOut::ShellCommandFailed: bash[migrate gitlab-rails database] (/opt/gitlab/embedded/cookbooks/cache/cookbooks/gitlab/resources/rails_migration.rb line 16) had an error: Mixlib::ShellOut::ShellCommandFailed: Expected process to exit with [0], but received '1'\n---- Begin output of \"bash\"  \"/tmp/chef-script20220705-15213-yfjykp\" ----\nSTDOUT: rake aborted!\nStandardError: An error has occurred, all later migrations canceled:\n\nExpected batched background migration for the given configuration to be marked as 'finished', but it is 'active':       {:job_class_name=>\"CopyColumnUsingBackgroundMigrationJob\", :table_name=>\"events\", :column_name=>\"id\", :job_arguments=>[[\"id\"], [\"id_convert_to_bigint\"]]}\n\nFinalize it manualy by running\n\n        sudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,events,id,'[[\"id\"]\\, [\"id_convert_to_bigint\"]]']\n\nFor more information, check the documentation\n\n        https://docs.gitlab.com/ee/user/admin_area/monitoring/background_migrations.html#database-migrations-failing-because-of-batched-background-migration-not-finished\n/opt/gitlab/embedded/service/gitlab-rails/lib/gitlab/database/migration_helpers.rb:1109:in `ensure_batched_background_migration_is_finished'\n/opt/gitlab/embedded/service/gitlab-rails/db/post_migrate/20210622045705_finalize_events_bigint_conversion.rb:11:in `up'\n/opt/gitlab/embedded/service/gitlab-rails/lib/gitlab/database/migrations/lock_retry_mixin.rb:31:in `ddl_transaction'\n/opt/gitlab/embedded/service/gitlab-rails/lib/tasks/gitlab/db.rake:61:in `block (3 levels) in <top (required)>'\n/opt/gitlab/embedded/bin/bundle:23:in `load'\n/opt/gitlab/embedded/bin/bundle:23:in `<main>'\n\nCaused by:\nExpected batched background migration for the given configuration to be marked as 'finished', but it is 'active':       {:job_class_name=>\"CopyColumnUsingBackgroundMigrationJob\", :table_name=>\"events\", :column_name=>\"id\", :job_arguments=>[[\"id\"], [\"id_convert_to_bigint\"]]}\n\nFinalize it manualy by running\n\n        sudo gitlab-rake gitlab:background_migrations:finalize[CopyColumnUsingBackgroundMigrationJob,events,id,'[[\"id\"]\\, [\"id_convert_to_bigint\"]]']\n\nFor more information, check the documentation\n\n        https://docs.gitlab.com/ee/user/admin_area/monitoring/background_migrations.html#database-migrations-failing-because-of-batched-background-migration-not-finished\n/opt/gitlab/embedded/service/gitlab-rails/lib/gitlab/database/migration_helpers.rb:1109:in `ensure_batched_background_migration_is_finished'\n/opt/gitlab/embedded/service/gitlab-rails/db/post_migrate/20210622045705_finalize_events_bigint_conversion.rb:11:in `up'\n/opt/gitlab/embedded/service/gitlab-rails/lib/gitlab/database/migrations/lock_retry_mixin.rb:31:in `ddl_transaction'\n/opt/gitlab/embedded/service/gitlab-rails/lib/tasks/gitlab/db.rake:61:in `block (3 levels) in <top (required)>'\n/opt/gitlab/embedded/bin/bundle:23:in `load'\n/opt/gitlab/embedded/bin/bundle:23:in `<main>'\nTasks: TOP => db:migrate\n(See full trace by running task with --trace)\n== 20210622045705 FinalizeEventsBigintConversion: migrating ===================\nSTDERR: \n---- End output of \"bash\"  \"/tmp/chef-script20220705-15213-yfjykp\" ----\nRan \"bash\"  \"/tmp/chef-script20220705-15213-yfjykp\" returned 1\n\n\nWarnings:\nThe version of the running redis service is different than what is installed.\nPlease restart redis to start the new version.\n\nsudo gitlab-ctl restart redis\n\nRunning handlers complete\nChef Infra Client failed. 2 resources updated in 24 seconds\n\nWarnings:\nThe version of the running redis service is different than what is installed.\nPlease restart redis to start the new version.\n\nsudo gitlab-ctl restart redis\n```\n\n![1657032630158](/gitlabUpdate/1657032630158.png)","tags":["gitlab"]},{"title":"docker-compose","url":"/2022/06/23/docker-compose/","content":"\n#### docker compose简介\n\nDocker-Compose 是用来管理容器的，类似用户容器管家，我们有N多台容器或者应用需要启动的时候，如果手动去操作，是非常耗费时间的，如果有了 Docker-Compose 只需要一个配置文件就可以帮我们搞定，但是 Docker-Compose 只能管理当前主机上的 Docker，不能去管理其他服务器上的服务。意思就是单机环境。\n\n\n\n#### docker compose安装（前提先安装docker）\n\n1.到官网下载docker compose文件 https://github.com/docker/compose/releases\n\n2.将文件上传至虚拟机/usr/bin目录,重命名为docker-compose\n\n3.修改docker-compose权限为可执行\n\n```\nchmod +x docker-compose\n```\n\n\n\n#### docker-compose.yml文件详解 \n\n包含四个一级key:version、services、networks、volumes\n\nversion:必备选项，指定了compose文件格式的版本\n\nservices:用于定义不同的应用服务，docker Compose会将每个服务部署到各自的容器中\n\nnetworks:用于指引docker创建新的网络。默认情况下会创建bridge网络。\n\nvolumes:用于指引docker来创建新的卷。\n\n\n\n下面是简单的dockercompose\n\n```yml\nversion: \"3.9\"\n\n# 服务 在它下面可以定义应用需要的一些服务，每个服务都有自己的名字、使用的镜像、挂载的数据卷、所属的网络、依赖哪些其他服务等等。\nservices:\n  #服务名称\n  webapp:\n    # docker 运行在名字，在docker ps -a 看到的名字\n    container_name: mynginx\n    # 镜像 : 标签\n    image: nginx:latest\n    # 端口映射 主机端口： 容器端口\n    ports:\n      - \"80:80\"\n    # 物理卷挂载 将本地路径挂载到容器内\n    volumes:\n      - D:/docker-compose/html:/usr/share/nginx/html\n    dns:\n      - 192.168.123.1\n```\n\n\n\n#### docker-compose的相关指令\n\n1.docker-compose up 启动一个docker-compose的应用，如果docker compose的配置文件名不是docker-compose.yml或者docker-compose.yaml，需要加-f来指定文件名，加-d来后台启动应用\n\ndocker-compose -f xxxxx.yml up -d\n\n2.docker-compose stop命令停止compose应用相关\n\n3.docker-compose rm 删除已经停止的容器，它们会删除容器和网络，但是不会删除卷和镜像\n\n4.docker-compose restart重启命令，如果需要使改变的内容生效，需要重新部署应用。\n\n5.docker-compose ps 列出应用中的容器\n\n6.docker-compose down 停止并删除运行中的容器，它会删除网络和容器，但是不会删除卷和镜像。\n\n","tags":["docker"]},{"title":"免密登录","url":"/2022/06/16/ssh-keygen/","content":"\n目的：为了方便服务器中间的切换，我们需要服务器之间做免密登录\n\n准备两台服务器\n\n```\n192.168.147.7 #生成秘钥的服务\n192.168.147.9 #远程登录的服务\n```\n\n第一步：在192.168.147.7的服务器上生成秘钥，使用ssh-keygen -t rsa命令，然后一直点确定\n\n```\n[root@localhost wen]# ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/root/.ssh/id_rsa): \n/root/.ssh/id_rsa already exists.\nOverwrite (y/n)? y\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /root/.ssh/id_rsa.\nYour public key has been saved in /root/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:YKlNYpZ2wTnqXm9APlc/w7RcBpwOconGk1BfGwN+mBA root@localhost.localdomain\nThe key's randomart image is:\n+---[RSA 2048]----+\n|     .o=E+.++.   |\n|     .+oB+++++   |\n|    *.*o +=oo.   |\n|   +.B..  ..o o  |\n|   ..o. S. = +   |\n|    . = .   B    |\n|   . . =     o   |\n|    .   o        |\n|       .         |\n+----[SHA256]-----+\n\n```\n\n第二步，将公钥文件上传到192.168.147.9的服务器上\n\n```\n#scp /root/.ssh/id_rsa.pub root@192.168.147.9:/root/.ssh/authorized_keys\n```\n\n第三步，更改公钥文件的权限为600\n\n```\nchmod 600 /root/.ssh/authorized_keys\n```\n\n第四步，远程测试\n\n```\nssh -l root 192.168.147.9\n```\n\n","tags":["linux"]},{"title":"计算机网络-数据链路层","url":"/2022/06/07/computerNetwork3/","content":"\n## 第三章\n\n数据链路层使用的信道主要有以下两种类\n\n(1) 点对点信道。这种信道使用一对一的点对点通信方式。\n\n(2) 广播信道。这种信道使用一对多的广播通信方式，因此过程比较复杂。广播信道上连接的主机很多，因此必须使用专用的共享信道协议来协调这些主机的数据发送。\n\n### 3.1点对点信道的数据链路层\n\n#### 3.1.1数据链路和帧\n\n“链路”和“数据链路”不是同一个概念\n\n所谓链路(link)就是从一个结点到相邻结点的一段物理线路（有线或无线），而中间没有任何其他的交换结点。\n\n数据链路(data link)另一个概念。这是因为当需要在一条线路上传送数据时，除了必须有一条物理线路外，还必须有一些必要的通信协议来控制这些数据的传输。\n\n\n\n总结：数据链路层把网络层传下来的ip数据报封装成帧，把封装好的帧发送给结点B的数据链路层，结点B收到的帧无差错，从帧中提取ip数据报交给网络层，否则丢弃这个帧。\n\n\n\n#### 3.1.2三个基本问题\n\n1.封装成帧\n\n2.透明传输\n\n3.差错检测\n\n### 3.2点对点协议ppp\n\n点对点协议PPP (Point-to-PointProtocol)则是目前使用得最广泛的数据链路层协议。\n\n#### 3.2.1PPP协议的特点\n\nPPP协议应满足的需求\n\n(1) 简单\n\n(2) 封装成帧\n\n(3) 透明性 \n\n(4) 多种网络层协议\n\n(5) 多种类型链路\n\n(6) 差错检测\n\n(7) 检测连接状态\n\n(8) 最大传送单元\n\n(9) 网络层地址协商\n\n(10) 数据压缩协商\n\n\n\nPPP协议的组成\n\n(1) 一个将IP数据报封装到串行链路的方法。\n\n(2) 一个用来建立、配置和测试数据链路连接的链路控制协议LCP (LinkControl Protocol)。\n\n(3) 一套网络控制协议NCP (Network Control Protocol)[插图]，其中的每一个协议支持不同的网络层协议，如IP、OSI的网络层、DECnet，以及AppleTalk等。","tags":["计算机网络"]},{"title":"Nginx安装（linux源码包安装）","url":"/2022/05/19/nginx_install/","content":"\n### 初始配置文件\n\n```shell\n\n#user  nobody;\nworker_processes  10;\n\n#error_log  logs/error.log;\n#error_log  logs/error.log  notice;\n#error_log  logs/error.log  info;\n\n#pid        logs/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    include       mime.types;\n    default_type  application/octet-stream;\n\n    #log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n    #                  '$status $body_bytes_sent \"$http_referer\" '\n    #                  '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    #access_log  logs/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    #keepalive_timeout  0;\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    server {\n        listen       80;\n        server_name  localhost;\n\n        #charset koi8-r;\n\n        #access_log  logs/host.access.log  main;\n\n        location / {\n            root   html;\n            index  index.html index.htm;\n\t        index  index.php;\n        }\n\n        #error_page  404              /404.html;\n\n        # redirect server error pages to the static page /50x.html\n        #\n        error_page   500 502 503 504  /50x.html;\n        location = /50x.html {\n            root   html;\n        }\n        location ~ \\.php$ {\n            root html;\n            fastcgi_pass   127.0.0.1:9000;\n            fastcgi_index  index.php;\n            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;\n            include        fastcgi_params;\n        }\n\n        # proxy the PHP scripts to Apache listening on 127.0.0.1:80\n        #\n        #location ~ \\.php$ {\n        #    proxy_pass   http://127.0.0.1;\n        #}\n\n        # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n        #\n        #location ~ \\.php$ {\n        #    root           html;\n        #    fastcgi_pass   127.0.0.1:9000;\n        #    fastcgi_index  index.php;\n        #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;\n        #    include        fastcgi_params;\n        #}\n\n        # deny access to .htaccess files, if Apache's document root\n        # concurs with nginx's one\n        #\n        #location ~ /\\.ht {\n        #    deny  all;\n        #}\n    }\n\n\n    # another virtual host using mix of IP-, name-, and port-based configuration\n    #\n    #server {\n    #    listen       8000;\n    #    listen       somename:8080;\n    #    server_name  somename  alias  another.alias;\n\n    #    location / {\n    #        root   html;\n    #        index  index.html index.htm;\n    #    }\n    #}\n\n\n    # HTTPS server\n    #\n    #server {\n    #    listen       443 ssl;\n    #    server_name  localhost;\n\n    #    ssl_certificate      cert.pem;\n    #    ssl_certificate_key  cert.key;\n\n    #    ssl_session_cache    shared:SSL:1m;\n    #    ssl_session_timeout  5m;\n\n    #    ssl_ciphers  HIGH:!aNULL:!MD5;\n    #    ssl_prefer_server_ciphers  on;\n\n    #    location / {\n    #        root   html;\n    #        index  index.html index.htm;\n    #    }\n    #}\n\n}\n\n```\n\n### 反向代理\n\n1)在hosts文件配置域名\n\n2)安装tomcat\n\n3)在nginx中配置指向地址\n\n```shell\n server {\n        listen       80;\n        server_name  www.234.com;\n\n        #charset koi8-r;\n\n        #access_log  logs/host.access.log  main;\n\n        location / {\n            root   html;\n            index  index.html index.htm;\n\t        index  index.php;\n\t        proxy_pass   http://127.0.0.1:8080; #反向代理\n        }\n```\n\n\n\n案例二\n\n在tomcat的webapps目录下创建edu和vod\n\n```\nserver {\n        listen       80;\n        server_name  www.234.com;\n\n        #charset koi8-r;\n\n        #access_log  logs/host.access.log  main;\n        #浏览器输入www.234.com/edu/a.html访问\n        location ~ /edu/ {\n            root   html;\n            index  index.html index.htm;\n\t        index  index.php;\n\t        proxy_pass   http://127.0.0.1:8080; #反向代理\n        }\n        #浏览器输入www.234.com/vod/a.html访问\n        location ~ /vod/ {\n            root   html;\n            index  index.html index.htm;\n\t        index  index.php;\n\t        proxy_pass   http://127.0.0.1:8081; #反向代理\n        }\n```\n\n","tags":["nginx"]},{"title":"VSCode集成GO语言(windows)","url":"/2022/04/29/VSCode/","content":"\n\n\n\n\n下载\n\nhttps://golang.google.cn/dl/go1.18.1.windows-amd64.msi\n\n安装，点击下一步下一步即可\n\n\n\n配置环境变量\n\nGOROOT\tD:\\Program Files\\Go\n\nPATH\t\tD:\\Program Files\\Go\\bin\n\nGOPATH\t\tgo的工作目录\n\n\n\n\n\n## go的第一个程序\n\n1.编写代码\n\n```go\npackage main\nimport \"fmt\"\n\nfunc main()  {\n\tfmt.Println(\"你好\")\n}\n```\n\n2.编译代码\n\ngo build test.go\n\n3.运行代码\n\ngo run test.go\n\n或\n\ntest.exe\n\n## go的转义字符\n\n```\n\\t #表示制表符tab\n\\n #换行符\n\\r #覆盖前面的内容\n\\\\ #输出字符\\\n\\\" #输出字符\"\n```\n\n## 注释\n\n```\n//  行注释\n注释快捷键 ctrl+/\n/*  */  块注释\n\n```\n\n## 快捷键\n\n```\n注释快捷键 ctrl+/\n代码对齐   shift + tab 整体向左移动\n格式化代码 gofmt -w test.go\n```\n\n## 变量\n\n```shell\nvar i int = 10 #定义赋值\n如果不赋值就会有个默认值\nint类型的默认值为0\n```\n\n","tags":["GO"]},{"title":"计算机网络-物理层","url":"/2022/04/28/computerNetwork2/","content":"\n\n\n#### 2.2 数据通信的基础知识\n\n##### 2.2.1 数据通信系统的模型\n\n数据通信系统可划分为三大部分，即源系统（或发送端、发送方）、传输系统（或传输网络）和目的系统（或接收端、接收方）。\n\n\n\n● 源点(source)：源点设备产生要传输的数据，例如，从PC的键盘输入汉字，PC产生输出的数字比特流。源点又称为源站，或信源。\n\n● 发送器：通常，源点生成的数字比特流要通过发送器编码后才能够在传输系统中进行传输。典型的发送器就是调制器。现在很多PC使用内置的调制解调器（包含调制器和解调器），用户在PC外面看不见调制解调器。目的系统一般也包括以下两个部分：\n\n● 接收器：接收传输系统传送过来的信号，并把它转换为能够被目的设备处理的信息。典型的接收器就是解调器，它把来自传输线路上的模拟信号进行解调，提取出在发送端置入的消息，还原出发送端产生的数字比特流。\n\n● 终点(destination)：终点设备从接收器获取传送来的数字比特流，然后把信息输出（例如，把汉字在PC屏幕上显示出来）。终点又称为目的站，或信宿。\n\n\n\n认识下面术语：\n\n1.通信的目的是传送消息(message)。如话音、文字、图像、视频等都是消息\n\n2.数据(data)是运送消息的实体，通常是有意义的符号序列；\n\n3.信号(signal)则是数据的电气或电磁的表现。\n\n4.模拟信号，或连续信号——代表消息的参数的取值是连续的。\n\n5.数字信号，或离散信号——代表消息的参数的取值是离散的。\n\n\n\n##### 2.2.2 有关信道的几个基本概念\n\n在许多情况下，我们要使用“信道(channel)”这一名词。信道和电路并不等同。信道一般都是用来表示向某一个方向传送信息的媒体。\n\n(1) 单向通信 又称为单工通信，即只能有一个方向的通信而没有反方向的交互。无线电广播或有线电广播以及电视广播就属于这种类型。\n\n(2) 双向交替通信 又称为半双工通信，即通信的双方都可以发送信息，但不能双方同时发送（当然也就不能同时接收）。这种通信方式是一方发送另一方接收，过一段时间后再反过来。\n\n(3) 双向同时通信 又称为全双工通信，即通信的双方可以同时发送和接收信息。","tags":["计算机网络"]},{"title":"prometheus自定义监控项","url":"/2022/04/26/prometheus_node_exporter/","content":"\n\n\nprometheus自定义监控\n\n由于官方给出的客户端监控模板是固定的，有时候我们想监控自己的指标，这个时候就要自己配置监控自己想监控的指标。首先你要安装好prometheus和node_exporter\n\n\n\n1.新建/usr/local/node_exporter/key目录\n\n2.编辑脚本\n\n```shell\n#!/bin/bash\necho nginx_procees $(ps -ef | grep nginx| grep -v grep | wc -l)\nexit 0\n```\n\n3.添加定时任务\n\n```shell\n*/1 * * * *  bash /usr/local/node_exporter/key/key_runner.sh > /usr/local/node_exporter/key/key.prom\n```\n\n4.重启，指定文件，程序就会自动到相关的目录下检测后缀名为prom的文件。\n\n```shell\nusr/local/node_exporter/node_exporter --collector.textfile.directory=/usr/local/node_exporter/key &\n```\n\n这时候我们访问接口就可以看到相关的输出结果\n\n![1650853696052](/prometheus_node_exporter/1650853696052.png)","tags":["prometheus"]},{"title":"prometheus+grafana+alertmanager报警","url":"/2022/04/16/prometheus/","content":"\n\n\n#### 环境搭建\n\n准备三台服务器：\n\nprometheus服务端\n\nprometheus客户端\n\nGrafana\n\n\n\n#### 同步时间\n\n三台服务器都要同步一下时间\n\n```shell\nyum install ntpdate -y\n```\n\n\n\n#### 下载安装prometheus\n\n下载地址：https://prometheus.io/download/，找到符合自己的环境的安装包下载即可，博主的测试环境是centos7。\n\n上传服务器，解压，然后移动目录到统一路径下方便管理，当然可以不移动\n\n```shell\ntar -zxvf prometheus-2.34.0.linux-amd64.tar.gz\nmv prometheus-2.34.0.linux-amd64 /usr/local/prometheus\n```\n\n启动prometheus，浏览器访问如下图，默认端口为9090\n\n```shell\n./prometheus --config.file=\"/usr/local/prometheus/prometheus.yml\" &\n```\n\n![1650849299406](/prometheus/1650849299406.png)\n\n##### 监控客户端主机\n\n如果你想采集其他服务器的信息，就需要在目标服务器上安装node_exporter插件，prometheus是通过配置node_exporter的插件地址来采集相应服务器的信息。我们可以把prometheus看做服务端，exporter相关的插件看做客户端。\n\n1.下载node_exporter:https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz\n\n2.安装node_exporter\n\n```shell\n#解压安装包\ntar -zxvf node_exporter-1.3.1.linux-amd64.tar.gz\n#更改目录名称\nmv node_exporter-1.3.1.linux-amd64 /usr/local/node_exporter\n```\n\n3.运行node_exporter\n\n```shell\nnohup /usr/local/node_exporter/node_exporter &\n```\n\n4.配置服务端，采集客户端数据\n\nvim /usr/local/prometheus/prometheus.yml\n\n```shell\n- job_name: 'agent'\n   static_configs:\n     - targets: ['192.168.147.8:9100']\n```\n\n5.重启prometheus。\n\n\n\n##### prometheus监控MySQL\n\n到prometheus官网下载相关的mysql插件https://github.com/prometheus/mysqld_exporter/releases/download/v0.14.0/mysqld_exporter-0.14.0.linux-amd64.tar.gz\n\n1.解压，重命名\n\n```shell\ntar -zxvf mysqld_exporter-0.14.0.linux-amd64.tar.gz\nmv mysqld_exporter-0.14.0.linux-amd64 /usr/local/mysqld_exporter\n```\n\n2.安装MySQL数据库，查看博主MySQL安装教程\n\n3.新建/usr/local/mysql_exporter/my.cnf文件，配置下面内容\n\n```shell\n[client]\nuser=root #数据库名\npassword=123456 #数据库密码\n```\n\n4.启动mysql_exporeter\n\n```shell\nnohup ./mysqld_exporeter --config.my-cnf=/usr/local/mysql_exporter/my.cnf &\n```\n\n5.服务器获取mysql_exporter\n\nvim /usr/local/prometheus/prometheus.yml\n\n```shell\n- job_name: 'mysql_exporter'\n  static_configs:\n  - targets: ['192.168.147.8:9104']\n```\n\n6.重启prometheus\n\n\n\n#### Grafana安装\n\n1.下载安装\n\n```shell\nwget https://dl.grafana.com/enterprise/release/grafana-enterprise-8.4.6-1.x86_64.rpm\nyum -y install grafana-enterprise-8.4.6-1.x86_64.rpm\n```\n\n2.启动服务\n\n```shell\nsystemctl start grafana-server\nsystemctl enable grafana-server\n```\n\n3.用浏览器访问界面，默认端口是3000,添加prometheus数据源，选中设置图标，点击Data sources\n\n![1650616079698](/prometheus/1650616079698.png)\n\n配置连接prometheus地址，点击保存即可\n\n![1650616200899](/prometheus/1650616200899.png)\n\n\n\n4.导入模板\n\n你可以到网上搜索相应的模板导入，或者直接搜索8919导入。该模板是服务端的系统信息的模板。\n\n![1650616353899](/prometheus/1650616353899.png)\n\n导入后效果如下：\n\n![1650616547405](/prometheus/1650616547405.png)\n\n\n\n#### alertmanager报警\n\n1.下载解压安装\n\n```shell\nwget https://github.com/prometheus/alertmanager/releases/download/v0.21.0/alertmanager-0.21.0.linux-amd64.tar.gz\ntar zxvf alertmanager-0.21.0.linux-amd64.tar.gz -C /usr/local/\nmv alertmanager-0.21.0.linux-amd64 /usr/local/alertmanager\ncd /usr/local/alertmanager\n./alertmanager  #启动\n```\n\n\n\n2.配置prometheus规则\n\nvim prometheus.yml,去掉注释\n\n```yaml\nrule_files:\n   - rule.yml\n```\n\n\n\n3.在prometheus根目录下创建rule.yml文件，我这里只是配置了进程状态监控\n\n```shell\nvim rule.yml\ngroups:\n- name: port_status\n  rules:\n  - alert: \"端口状态监测\"\n    expr: probe_success{job=\"port_status\"} == 0\n    for: 1s\n    labels:\n      severity: \"High\"\n    annotations:\n      description: \"Instance {{ $labels.instance }} 端口down掉\"\n      value: \"{{ $value }}\"\n\n- name: web_status\n  rules:\n  - alert: \"网站状态监测\"\n    expr: probe_http_status_code  == 0\n    for: 1s\n    labels:\n      severity: \"High\"\n    annotations:\n      description:  \"Instance {{ $labels.instance }} 网站无法访问\"\n      value: \"{{ $value }}\"\n\n- name: process\n  rules:\n  - alert: \"进程状态监测\"\n    expr: namedprocess_namegroup_num_procs  == 0\n    for: 1s\n    labels:\n      severity: \"High\"\n    annotations:\n      description: \"Instance {{ $labels.instance }} 服务进程死亡!\"\n      value: \"{{ $value }}\"\n```\n\n4.进入alertmanager根目录，编辑alertmanager.yml，配置如下\n\n```yaml\nglobal:\n   resolve_timeout: 5m\n  ## 这里为qq邮箱 SMTP 服务地址，官方地址为 smtp.qq.com 端口为 465 或 587，同时要设置开启 POP3/SMTP 服务。\n   smtp_smarthost: 'smtp.qq.com:465'\n   smtp_from: '952635xxx@qq.com'\n   smtp_auth_username: '952635xxx@qq.com'\n   #授权码，不是密码,在 QQ 邮箱服务端设置开启 POP3/SMTP 服务时会提示\n   smtp_auth_password: 'psejbvpfrqfebdhb'\n   smtp_require_tls: false\n \n#1、模板\n#templates:\n#   - '/usr/local/alertmanager/templates/alert.tmpl'\n \n#2、路由\nroute:\n   group_by: ['alertname']\n   group_wait: 10s\n   group_interval: 10s\n   repeat_interval: 1h\n   #邮箱\n   receiver: 'email'\n \nreceivers:\n   - name: 'email'\n     email_configs:\n        - to: '952635xxx@qq.com'\n          send_resolved: true\n```\n\n\n\n5.检测配置文件是否正常\n\n```shell\n/usr/apps/alertmanager/amtool check-config /usr/apps/alertmanager/alertmanager.yml\n```\n\n6.重启prometheus和alertmanager，打开浏览器如下说明成功，然后停掉相关进程看看是否有邮件接受到即可。\n\n![1650850067207](/prometheus/1650850067207.png)\n\n\n\n\n\n\n\n","tags":["prometheus"]},{"title":"tomcat安装配置（linux）","url":"/2022/04/16/tomcat01/","content":"\n安装步骤：\n\n1.去官网下载相应的安装包\n\n2.上传服务器解压\n\n3.配置后台登录\n\n4.测试\n\n#### 去官网下载相应的安装包\n\n安装tomcat之前要先安装jdk,jdk安装步骤可以查看本博客相应的文章\n\n```shell\nhttps://dlcdn.apache.org/tomcat/tomcat-9/v9.0.62/bin/apache-tomcat-9.0.62.tar.gz\n```\n\n#### 上传服务器解压\n\n```shell\nrz apache-tomcat-9.0.62.tar.gz\ntar -zxvf apache-tomcat-9.0.62.tar.gz\n```\n\n#### 配置后台登录\n\n编辑conf/tomcat-user.xml文件，内容如下：\n\n```shell\n<role rolename=\"manager-gui\"/>\n<user username=\"tomcat\" password=\"s3cret\" roles=\"manager-gui\"/>\n```\n\n![1650002701833](/tomcat01/1650002701833.png)\n\n取消tomcat的ip限制，进入tomcat文件夹下的webapps/manager/META-INF/context.xml将下面内容注释掉\n\n```shell\n<Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n         allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" />\n```\n\n#### 测试\n\n浏览器访问如下所示证明是安装完成\n\n![1650003158927](/tomcat01/1650003158927.png)\n\n","tags":["tomcat"]},{"title":"jdk安装配置(linux)","url":"/2022/04/15/java01/","content":"\n安装步骤：\n\n1.到官网下载安装包\n\n2.解压安装包\n\n3.配置环境变量\n\n4.测试\n\n\n\n#### 下载安装包\n\n到官网下载相应的安装包\n\n```shell\nhttps://www.oracle.com/java/technologies/downloads\n```\n\n#### 解压安装包\n\n将下载的安装包上传至服务器，并解压\n\n```shell\ntar -zxvf jdk1.8.0_201.tar.gz\n```\n\n![1649920162227](/java01/1649920162227.png)\n\n#### 配置环境变量\n\n编辑profile文件，在尾部添加如下信息，注意JAVA_HOME修改成自己的路径哦\n\n```shell\nvim /etc/profile\n\nexport JAVA_HOME=/opt/java/jdk1.8.0_201\nexport CLASSPATH=.:${JAVA_HOME}/jre/lib/rt.jar:${JAVA_HOME}/lib/dt.jar:\n${JAVA_HOME}/lib/tools.jar #这一行和上面一行在同一行\n\nexport PATH=$PATH:${JAVA_HOME}/bin\n```\n\n\n\n#### 测试\n\n执行如下两条指令，如图说明成功\n\n```shell\nsource /etc/profile\njava -version\n```\n\n![1649920851887](/java01/1649920851887.png)","tags":["java"]},{"title":"硬盘扩容","url":"/2022/04/13/Hard_drive_capacity/","content":"\n### 目标\n\n使用df -TH命令查看硬盘使用情况，我们现在想扩容/dev/mapper/centos-root分区的容量\n\n```shell\n[root@localhost ~]# df -TH\n文件系统                类型      容量  已用  可用 已用% 挂载点\n/dev/mapper/centos-root xfs       8.6G  5.7G  3.0G   67% /\ndevtmpfs                devtmpfs  945M     0  945M    0% /dev\ntmpfs                   tmpfs     956M     0  956M    0% /dev/shm\ntmpfs                   tmpfs     956M  9.0M  947M    1% /run\ntmpfs                   tmpfs     956M     0  956M    0% /sys/fs/cgroup\n/dev/sda1               xfs       1.1G  150M  914M   15% /boot\ntmpfs                   tmpfs     192M     0  192M    0% /run/user/0\n```\n\n\n\n### 一、前期准备\n\n（1）打开虚拟机添加一块硬盘\n\n![1652669941975](/Hard_drive_capacity/1652669941975.png)\n\n一直点击下一步即可，到磁盘大小我们改成5G即可\n\n![1652670321561](/Hard_drive_capacity/1652670098139.png)\n\n点击下一步，然后保存即可，最后打开虚拟机\n\n\n\n### 二、硬盘分区，格式化\n\nyum -y install parted 安装分区命令\n\n使用lsblk 命令查看刚刚添加的硬盘，如下图sdb就是我们刚刚添加的硬盘5G，现在我们先对它进行磁盘分区\n\n```\n[root@localhost ~]# lsblk\nNAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsda               8:0    0   10G  0 disk \n├─sda1            8:1    0    1G  0 part /boot\n└─sda2            8:2    0    9G  0 part \n  ├─centos-root 253:0    0    8G  0 lvm  /\n  └─centos-swap 253:1    0    1G  0 lvm  [SWAP]\nsdb               8:16   0    5G  0 disk \nsr0              11:0    1  4.2G  0 rom  \n```\n\n由于生产环境一般都是大于2T的扩容，我们就直接使用parted命令进行分区\n\n```\n[root@localhost ~]# parted /dev/sdb\nGNU Parted 3.1\n使用 /dev/sdb\nWelcome to GNU Parted! Type 'help' to view a list of commands.\n(parted) \n```\n\n输入m查看所以的命令\n\n```\n[root@localhost ~]# parted /dev/sdb\nGNU Parted 3.1\n使用 /dev/sdb\nWelcome to GNU Parted! Type 'help' to view a list of commands.\n(parted) m                                                                \n  align-check TYPE N                        check partition N for TYPE(min|opt) alignment\n  help [COMMAND]                           print general help, or help on COMMAND\n  mklabel,mktable LABEL-TYPE               create a new disklabel (partition table)\n  mkpart PART-TYPE [FS-TYPE] START END     make a partition\n  name NUMBER NAME                         name partition NUMBER as NAME\n  print [devices|free|list,all|NUMBER]     display the partition table, available devices, free space, all found partitions, or a particular partition\n  quit                                     exit program\n  rescue START END                         rescue a lost partition near START and END\n  rm NUMBER                                delete partition NUMBER\n  select DEVICE                            choose the device to edit\n  disk_set FLAG STATE                      change the FLAG on selected device\n  disk_toggle [FLAG]                       toggle the state of FLAG on selected device\n  set NUMBER FLAG STATE                    change the FLAG on partition NUMBER\n  toggle [NUMBER [FLAG]]                   toggle the state of FLAG on partition NUMBER\n  unit UNIT                                set the default unit to UNIT\n  version                                  display the version number and copyright information of GNU Parted\n(parted)\n```\n\n输入mklabel gpt创建磁盘标签为GPT\n\n```shell\n[root@localhost ~]# parted /dev/sdb\nGNU Parted 3.1\n使用 /dev/sdb\nWelcome to GNU Parted! Type 'help' to view a list of commands.\n(parted) m                                                                \n  align-check TYPE N                        check partition N for TYPE(min|opt) alignment\n  help [COMMAND]                           print general help, or help on COMMAND\n  mklabel,mktable LABEL-TYPE               create a new disklabel (partition table)\n  mkpart PART-TYPE [FS-TYPE] START END     make a partition\n  name NUMBER NAME                         name partition NUMBER as NAME\n  print [devices|free|list,all|NUMBER]     display the partition table, available devices, free space, all found partitions, or a particular partition\n  quit                                     exit program\n  rescue START END                         rescue a lost partition near START and END\n  rm NUMBER                                delete partition NUMBER\n  select DEVICE                            choose the device to edit\n  disk_set FLAG STATE                      change the FLAG on selected device\n  disk_toggle [FLAG]                       toggle the state of FLAG on selected device\n  set NUMBER FLAG STATE                    change the FLAG on partition NUMBER\n  toggle [NUMBER [FLAG]]                   toggle the state of FLAG on partition NUMBER\n  unit UNIT                                set the default unit to UNIT\n  version                                  display the version number and copyright information of GNU Parted\n(parted) mklabel gpt                                                      \n(parted)\n```\n\n使用mkpart primary 0% 100%划分所以空间到一个分区，划分后我们使用print打印出来即可看到结果\n\n```shell\n(parted) print                                                            \nModel: VMware, VMware Virtual S (scsi)\nDisk /dev/sdb: 5369MB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start  End  Size  File system  Name  标志\n\n(parted) mkpart primary 0% 100%                                           \n(parted) print                                                            \nModel: VMware, VMware Virtual S (scsi)\nDisk /dev/sdb: 5369MB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start   End     Size    File system  Name     标志\n 1      1049kB  5368MB  5367MB               primary\n\n(parted)\n```\n\n使用quit命令退出完成分区\n\n接下来使用mkfs.xfs  /dev/sdb1命令进行格式化，再使用lsblk命令查看我们分了一个名为sdb1的分区\n\n```shell\n[root@localhost ~]# lsblk\nNAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsda               8:0    0   10G  0 disk \n├─sda1            8:1    0    1G  0 part /boot\n└─sda2            8:2    0    9G  0 part \n  ├─centos-root 253:0    0    8G  0 lvm  /\n  └─centos-swap 253:1    0    1G  0 lvm  [SWAP]\nsdb               8:16   0    5G  0 disk \n└─sdb1            8:17   0    5G  0 part \nsr0              11:0    1  4.2G  0 rom\n```\n\n格式化\n\n```shell\n[root@localhost ~]# mkfs.xfs  /dev/sdb1\nmeta-data=/dev/sdb1              isize=512    agcount=4, agsize=327552 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n         =                       crc=1        finobt=0, sparse=0\ndata     =                       bsize=4096   blocks=1310208, imaxpct=25\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1\nlog      =internal log           bsize=4096   blocks=2560, version=2\n         =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\n[root@localhost ~]#\n```\n\n### 三、创建物理分区，逻辑分区\n\n格式化完成了，这是你就可以使用mount /dev/sdb1 /data将该分区挂载到任意目录上面，但是我们这里是要扩容原有的逻辑分区，还需要下面几个步骤\n\n使用pvcreate /dev/sdb1创建物理分区，提示成功即可，也可以使用pvs查看\n\n```shell\n[root@localhost ~]# pvcreate /dev/sdb1\nWARNING: xfs signature detected on /dev/sdb1 at offset 0. Wipe it? [y/n]: y\n  Wiping xfs signature on /dev/sdb1.\n  Physical volume \"/dev/sdb1\" successfully created.\n[root@localhost ~]# \n```\n\n使用vgextend centos /dev/sdb1命令添加到vg中，其中vg名称可以通过lvs查看你要扩容的分区vg的名称\n\n```shell\n[root@localhost ~]# lvs\n  LV   VG     Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n  root centos -wi-ao---- <8.00g                                                    \n  swap centos -wi-ao----  1.00g                                                    \n[root@localhost ~]# vgextend centos /dev/sdb1\n  Volume group \"centos\" successfully extended\n[root@localhost ~]# \n```\n\n将所以的容量添加到分区中lvextend -l +100%FREE /dev/mapper/centos-root\n\n```shell\n[root@localhost ~]# lvextend -l +100%FREE /dev/mapper/centos-root\n  Size of logical volume centos/root changed from <8.00 GiB (2047 extents) to 12.99 GiB (3326 extents).\n  Logical volume centos/root successfully resized.\n[root@localhost ~]# \n```\n\n最后实现扩容\nxfs格式的使用下面命令，这里我是xfs格式我就使用这个命令\n\n```shell\n[root@localhost ~]# xfs_growfs /dev/mapper/centos-root\nmeta-data=/dev/mapper/centos-root isize=512    agcount=4, agsize=524032 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n         =                       crc=1        finobt=0 spinodes=0\ndata     =                       bsize=4096   blocks=2096128, imaxpct=25\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1\nlog      =internal               bsize=4096   blocks=2560, version=2\n         =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\ndata blocks changed from 2096128 to 3405824\n[root@localhost ~]#\n```\n\next4:格式使用下面命令\nresize2fs -f /dev/mapper/cl-root\n\n\n\n使用df -TH 查看,可以看到扩容成功\n\n```shell\n[root@localhost ~]# df -TH\n文件系统                类型      容量  已用  可用 已用% 挂载点\n/dev/mapper/centos-root xfs        14G  5.7G  8.3G   41% /\ndevtmpfs                devtmpfs  945M     0  945M    0% /dev\ntmpfs                   tmpfs     956M     0  956M    0% /dev/shm\ntmpfs                   tmpfs     956M  9.0M  947M    1% /run\ntmpfs                   tmpfs     956M     0  956M    0% /sys/fs/cgroup\n/dev/sda1               xfs       1.1G  150M  914M   15% /boot\ntmpfs                   tmpfs     192M     0  192M    0% /run/user/0\n[root@localhost ~]# \n```\n\n","tags":["linux"]},{"title":"计算机网络-概述","url":"/2022/04/13/computerNetwork1/","content":"\n\n\n### 第一章\n\n#### 概述\n\n网络：这里指“三网”，即电信网络、有线电视网络和计算机网络。\n\n网络融合：随着技术的发展，电信网络和有线电视网络都逐渐融入了现代计算机网络的技术，这就产生了“网络融合”\n\n资源共享：计算机网络上有许多主机存储了大量有价值的电子文档，可供上网的用户自由读取或下载（无偿或有偿），这些资源好像就在用户身边一样。\n\n#### 因特网\n\n先认识网络，因特网，互联网这三个概念。\n\n网络：由若干结点(node)[插图]和连接这些结点的链路(link)组成。\n\n互联网：网络和网络还可以通过路由器互连起来，这样就构成了一个覆盖范围更大的网络，即互联网\n\n因特网：是世界上最大的互连网络（用户数以亿计，互连的网络数以百万计）。\n\n![1649727589146](/computerNetwork1.1/1649727589146.png)\n\n\n\n网络把许多计算机连接在一起，而因特网则把许多网络连接在一起。\n\n因特网就是世界上最大的计算机网络。\n\n#### 因特网发展的三个阶段\n\n1.第一阶段是从单个网络ARPANET向互联网发展的过程。\n\n​        1983年，TCP/IP协议成为ARPANET上的标准协议，使得所有使用TCP/IP协议的计算机都能利用互连网相互通信，因而人们就把1983年作为因特网的诞生时间。1990年，ARPANET正式宣布关闭，因为它的实验任务已经完成。\n\ninternet和Internet的区别。\n\n​        internet（互联网或互连网）是一个通用名词，它泛指由多个计算机网络互连而成的网络\n\n​        Internet（因特网）则是一个专用名词，它指当前全球最大的、开放的、由众多网络相互连接而成的特定计算机网络，它采用TCP/IP协议族作为通信的规则，且其前身是美国的ARPANET。\n\n2.第二阶段的特点是建成了三级结构的因特网。\n\n​       它是一个三级计算机网络，分为主干网、地区网和校园网（或企业网）。\n\n3.第三阶段的特点是逐渐形成了多层次 ISP 结构的因特网。\n\n​        ISP:因特网服务提供者ISP(Internet Service Provider)。中国电信、中国联通和中国移动就是我国最有名的ISP。\n\n![epub_655484_6](/computerNetwork1.1/epub_655484_6.png)\n\n#### 因特网的组成\n\n边缘部分：由所有连接在因特网上的主机组成。这部分是用户直接使用的，用来进行通信（传送数据、音频或视频）和资源共享。\n\n核心部分：由大量网络和连接这些网络的路由器组成。这部分是为边缘部分提供服务的（提供连通性和交换）。\n\n![epub_655484_12](/computerNetwork1.1/epub_655484_12.png)\n\n##### 边缘部分\n\n在网络边缘的端系统之间的通信方式通常可划分为两大类：客户-服务器方式（C/S方式）和对等方式（P2P方式）\n\n客户是服务请求方，服务器是服务提供方。\n\n客户程序：(1) 被用户调用后运行，在通信时主动向远地服务器发起通信（请求服务）。因此，客户程序必须知道服务器程序的地址。(2) 不需要特殊的硬件和很复杂的操作系统。服务器程序：(1) 是一种专门用来提供某种服务的程序，可同时处理多个远地或本地客户的请求。(2) 系统启动后即自动调用并一直不断地运行着，被动地等待并接受来自各地的客户的通信请求。因此，服务器程序不需要知道客户程序的地址。(3) 一般需要有强大的硬件和高级的操作系统支持。客户与服务器的通信关系建立后，通信可以是双向的，客户和服务器都可发送和接收数据。\n\n\n\n对等连接方式从本质上看仍然是使用客户-服务器方式，只是对等连接中的每一个主机既是客户又同时是服务器。\n\n##### 核心部分\n\n​       在网络核心部分起特殊作用的是路由器（router），它是一种专用计算机（但不是主机）。路由器是实现分组交换（packet switching）的关键构件，其任务是转发收到的分组，这是网络核心部分最重要的功能。\n\n\n\n三种交换方式：\n\n电路交换：整个报文的比特流连续地从源点直达终点，好像在一个管道中传送。这种必须经过“建立连接（占用通信资源）→通话（一直占用通信资源）→释放连接（归还通信资源）”三个步骤的交换方式称为电路交换。电路交换的一个重要特点就是在通话的全部时间内，通话的两个用户始终占用端到端的通信资源。\n\n分组交换：单个分组（这只是整个报文的一部分）传送到相邻结点，存储下来后查找转发表，转发到下一个结点。\n\n​\t报文：通常我们把要发送的整块数据称为一个报文\n\n​\t分组：先把较长的报文划分成为一个个更小的等长数据段，例如，每个数据段为1024bit[插图]。在每一个数据段前面，加上一些必要的控制信息组成的首部(header)后，就构成了一个分组(packet)。分组又称为“包”，而分组的首部也可称为“包头”。\n\n\n\n报文交换：整个报文先传送到相邻结点，全部存储下来后查找转发表，转发到下一个结点。\n\n\n\n#### 几种不同类别的网络\n\n按网络的作用范围进行分类\n\n广域网\n\n城域网\n\n局域网\n\n个人区域网\n\n按网络的使用者进行分类\n\n公用网\n\n专用网\n\n用来把用户接入到因特网的网络\n\n接入网\n\n#### 计算机网络的性能指标\n\n速率：网络技术中的速率指的是连接在计算机网络上的主机在数字信道上传送数据的速率，它也称为数据率(data rate)或比特率(bit rate)。\n\n带宽：表示通信线路允许通过的信号频带范围就称为线路的带宽\n\n吞吐量：表示在单位时间内通过某个网络（或信道、接口）的数据量。\n\n时延：是指数据（一个报文或分组，甚至比特）从网络（或链路）的一端传送到另一端所需的时间\n\n#### 计算机网络的非性能指标\n\n费用：\n\n质量：\n\n标准化：\n\n可靠性：\n\n可扩展性和可升级性：\n\n易于管理和维护：\n\n#### 计算机网络体系结构的形成\n\n原因：为了解决不同产品使用不同的网络体系，是不同设备间无法相互连通，导致市场出现垄断现象。\n\n国际标准化组织ISO提出了（开放系统互连基本参考模型）OSI的标准，但是由于现实的问题一直停留在理论上，没有实际应用到实际中。得到广泛应用的是非国际标准的TCP/IP。\n\n##### 协议与划分层次\n\n语法：即数据与控制信息的结构或格式；\n\n语义：即需要发出何种控制信息，完成何种动作以及做出何种响应；\n\n同步：即事件实现顺序的详细说明。\n\n\n\n分层的优点：\n\n(1) 各层之间是独立的\n\n(2) 灵活性好\n\n(3) 结构上可分割开\n\n(4) 易于实现和维护\n\n(5) 能促进标准化工作\n\n\n\n我们把计算机网络的各层及其协议的集合，称为网络的体系结构\n\n\n\n#### 五层协议的体系结构\n\nOSI的七层协议体系结构的概念清楚，理论也较完整，但它既复杂又不实用。目前使用最广泛的是TCP/IP的体系结构，为了方便学习，我们采用折中的做法，采用只有五层的体系结构\n\n![epub_655484_12](/computerNetwork1.1/epub_655484_37.png)\n\n\n\n应用层：应用层是体系结构中的最高层。应用层的任务是通过应用进程间的交互来完成特定网络应用。\n\n运输层：运输层的任务就是负责向两个主机中进程之间的通信提供通用的数据传输服务。\n\n网络层：网络层负责为分组交换网上的不同主机提供通信服务。\n\n数据链路层：在两个相邻结点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧（framing），在两个相邻结点间的链路上传送帧（frame）。每一帧包括数据和必要的控制信息（如同步信息、地址信息、差错控制等）。\n\n物理层：如双绞线、同轴电缆、光缆、无线信道等，\n\n","tags":["计算机网络"]},{"title":"elasticsearch安装(centos7)","url":"/2022/04/13/elasticsearch_install/","content":"\n安装elasticsearch的前提是先安装jdk，如何安装jdk可以翻看博主的jdk安装教程\n\n### 1.下载安装包\n\nhttps://www.elastic.co/cn/downloads/elasticsearch\n\n选择符合自己版本的elasticsearch,我的虚拟机版本是centos7,64位\n\nhttps://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.2.2-linux-x86_64.tar.gz\n\n![1654669838953](/elasticsearch_install/1654669838953.png)\n\n### 2.上传并解压安装包\n\n```shell\ntar -zxvf elasticsearch-8.2.2-linux-x86_64.tar.gz\n```\n\n### 3.修改配置文件\n\n进入elasticsearch-8.2.2/config/，主要关注elasticsearch.yml和jvm.options这两个配置文件\n\n打开jvm.options，调整内存大小，根据自己的虚拟机内存的实际情况去配置，大概为机子内存的一半\n\n![1654670443830](/elasticsearch_install/1654670443830.png)\n\n\n\n打开elasticsearch.yml，配置主机ip和端口号，并在原来的基础上添加下面两行\n\n```\ndiscovery.seed_hosts: [\"127.0.0.1\"]\ncluster.initial_master_nodes: [\"node-1\"]\n```\n\n![1654671798044](/elasticsearch_install/1654671798044.png)\n\n### 4.创建elasticsearch用户，并授权\n\n```\ngroupadd es\n\nuseradd es -g es -p es\n\nchown -R es:es elasticsearch-8.2.2\n```\n\n\n\n### 5.还要修改系统文件的配置，配置如下\n\n```shell\n1.修改/etc/sysctl.conf\n文件最后添加如下内容\nvm.max_map_count=262144\n添加完成后运行如下命令\nsysctl -p\n\n2.修改/etc/security/limits.conf\n添加如下内容\n* hard nofile 65536\n* soft nofile 65536\n* soft nproc 2048\n* hard nproc 4096\n\n3.修改/etc/security/limits.d/90-nproc.conf\n修改如下内容：\n* soft nproc 1024\n修改为\n* soft nproc 4096\n————————————————\n```\n\n### 6.启动elasticsearch，到bin目录，启动指令如下：\n\n```\n./elasticsearch\n./elasticsearch -d #后台启动\n```\n\n打开浏览器输入ip+端口，如下说明成功了\n\n![1654680144517](/elasticsearch_install/1654680144517.png)\n\n### 7.设置密码\n\n在配置文件中开启x-pack验证，elasticsearch.yml文件，在里面添加如下内容,并重启.\n\n```shell\nxpack.security.enabled: true\nxpack.license.self_generated.type: basic\nxpack.security.transport.ssl.enabled: true\n```\n\n如有下面的报错\n\n```shell\nUnexpected response code [503] from calling PUT http://192.168.1.200:9200/_security/user/apm_system/_password?pretty\nCause: Cluster state has not been recovered yet, cannot write to the security index\n\nPossible next steps:\n* Try running this tool again.\n* Try running with the --verbose parameter for additional messages.\n* Check the elasticsearch logs for additional error details.\n* Use the change password API manually. \n\nERROR: Failed to set password for user [apm_system].\n```\n\n解决方法：\n\n```shell\n在elasticsearch.yml 配置文件中去除\ndiscovery.seed_hosts 和cluster.initial_master_nodes. \n然后添加discovery.type: single-node\n```\n\n执行设置用户名和密码的命令,这里需要为4个用户分别设置密码，elastic, kibana, logstash_system,beats_system\n\n```shell\nbin/elasticsearch-setup-passwords interactive\n```\n\n![1659012666752](/elasticsearch_install/1659012666752.png)","tags":["elasticsearch"]},{"title":"MySQL安装（linux源码包安装）","url":"/2022/04/12/mysql-install/","content":"\n安装步骤：\n\n1.解压、创建相关目录，环境检查\n\n2.创建用户用户组，授权\n\n3.编译\n\n4.修改配置文件my.cnf\n\n5.设置开机启动，配置环境变量等\n\n#### 解压、创建相关目录\n\n1.去官网下载相关的安装包：https://dev.mysql.com/downloads/mysql/\n\n2.解压\n\n```shell\ntar -zxvf mysql-5.7.13-linux-glibc2.5-x86_64.tar.gz\n```\n\n3.将解压的文件移动至/usr/local/mysql\n\n```shell\nmv mysql-5.7.13-linux-glibc2.12-x86_64 /usr/local/mysql\n```\n\n4.在/usr/local/mysql创建data目录\n\n```shell\ncd /usr/local/mysql\nmkdir data\n```\n\n5.查看是否有旧的数据库\n\n```\nrpm -qa | grep mariadb\nrpm -e 包名 --nodeps\n```\n\n\n\n#### 创建用户用户组，授权\n\n1.创建用户组\n\n```shell\ngroupadd mysql\n```\n\n2.添加用户，指定所属组\n\n```shell\nuseradd -r -g mysql mysql\n```\n\n3.授权\n\n```shell\nchown –R mysql:mysql /usr/local/mysql\n```\n\n#### 编译\n\n注意编译完成后记录末尾的密码，等下登录用到！\n\n```shell\n./mysqld --user=mysql --initialize --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data\n```\n\n![1](/mysql-install/1.png)\n\n#### 修改配置文件my.cnf\n\n编辑/etc/my.cnf，添加如下配置。\n\n```shell\nvim /etc/my.cnf\n[mysqld]\ndatadir=/usr/local/mysql/data\nsocket=/var/lib/mysql/mysql.sock\nbasedir=/usr/local/mysql\n```\n\n#### 设置开机启动，配置环境变量等\n\n1.进入/usr/local/mysql，做如下操作设置开机自启动\n\n```shell\ncp support-files/mysql.server /etc/init.d/mysql\nchmod +x /etc/init.d/mysql  #授执行权限\nchkconfig --add mysql #添加自动启动\nchkconfig --list #查看是否添加成功\n```\n\n2.配置环境变量\n\n```shell\nexport PATH=$PATH:/usr/local/mysql/bin:/usr/local/mysql/lib\nsource /etc/profile #重新加载文件\nsystemctl restart mysql #启动mysql数据库\nset password for root@localhost = password('123456'); #登录mysql，修改密码\n```\n\n![2](/mysql-install/2.png)","tags":["MySQL"]},{"title":"hexo图片不显示的问题","url":"/2022/04/08/hexo-images/","content":"\n\n\n\n\n问题：发现hexo上传图片的时候，界面无法加载图片\n\n解决思路：\n\n1.修改博客的根目录配置文件_config.yml\n\n2.安装hexo-asset-image插件\n\n3.新建文章，将图片复制到对应生成的文件夹中，文章中引入图片的相对路径\n\n4.重新加载hexo，重启项目\n\n#### 修改_config.yml配置文件\n\n将post_asset_folder改为true\n\n![1649325088239](/1649325088239.png)\n\n#### 安装插件\n\n```shell\nnpm install https://github.com/CodeFalling/hexo-asset-image --save\n```\n\n#### 新建文章\n\n```shell\nhexo new xxxx\n\n#将图片放入到_post文件下生成的文件夹\n\n#图片路径格如下\n![图片名](./xxxx/图片名.png)\n```\n\n#### 重新加载项目，启动项目\n\n```shell\nhexo g #重新加载\nhexo s #启动项目\n```\n\n\n\n需要注意的一点是，文件夹的不要使用中文命名","tags":["hexo"]},{"title":"win+hexo+github的初始化-搭建","url":"/2022/04/07/hexo-init/","content":"\n\n\n安装搭建步骤：\n\n1.下载安装node.js，github\n\n2.安装，初始化hexo\n\n3.github相关配置\n\n4.hexo发布到github\n\n#### 下载安装node.js，github\n\n1.1下载安装node.js，我这里演示的是windows环境下，其他系统的伙伴可以下载相应的版本，安装都差不多。\n\n```shell\nhttps://nodejs.org/dist/v16.14.0/node-v16.14.0-x64.msi\n```\n\n1.2下载安装github\n\n```shell\nhttps://github.com/git-for-windows/git/releases/download/v2.35.1.windows.2/Git-2.35.1.2-64-bit.exe\n```\n\n1.3检查是否安装成功，打开电脑cmd窗口,如下图\n\n```shell\nnode -v\nnpm -v\ngit --version\n```\n\n![1649399456021](/hexo-init/1649399456021.png)\n\n#### 安装，初始化hexo\n\n2.1安装hexo\n\n```shell\n#安装hexo\nnpm install hexo-cli -g\n#检查hexo\nhexo -v\n```\n\n![1649400337591](/hexo-init/1649400337591.png)\n\n2.2初始化hexo项目\n\n```shell\nhexo init #初始化项目\nhexo s\t  #启动项目\n```\n\n![1649401460373](/hexo-init/1649401460373.png)\n\n如上图，可以通过本地浏览器访问http://localhost:4000，按ctrl+c退出\n\n![](/hexo-init/123456.jpeg)\n\n#### github相关配置\n\n如果还没有github账号的先注册，\n\n3.1创建github仓库\n\n仓库名一定是你的用户名+github.io,然后点击确定即可\n\n![1649403419766](/hexo-init/1649403419766.png)\n\n3.2设置ssh-keys认证\n\n右键选择GIT bash打开命令窗口，输入一下命令生成密钥\n\n```shell\nssh-keygen -t rsa -C \"你的邮箱地址\"\n```\n\n![1649404089689](/hexo-init/1649404089689.png)\n\n密钥生成路径在c盘的adminstrator下有个.ssh，id_rsa就是密钥文件\n\n将公钥文件id_rsa.pub的内容复制到github的setting-ssh and GPG keys,点击new ssh key，添加后如下图\n\n![1649405201216](/hexo-init/1649405201216.png)\n\n3.3测试ssh绑定是否成功，如下提示表示成功\n\n```shell\nssh -T git@github.com\n```\n\n![1649405464377](/hexo-init/1649405464377.png)\n\n#### 线上发布博客\n\n4.1在hexo根目录打开git命令窗口，先设置git的邮箱和用户名\n\n```shell\ngit config --global user.email \"xxx@example.com\"\ngit config --global user.name \"xxx\"\n```\n\n4.2打开更目录_config.yml文件，注释掉原来的deploy配置，\n\n```shell\ndeploy:\n type: git\n #这个为你的地址,github仓库上可以找到\n repository: https://github.com/gechangjia/gechangjia.github.io.git\n branch: main\n```\n\n4.3安装hexo-deployer-git\n\n```shell\nnpm install hexo-deployer-git --save\n```\n\n4.4生成静态文件和部署，到这一步就完成了\n\n```shell\nhexo g\nhexo d\n```\n\n访问地址：https://+你的仓库名","tags":["hexo"]},{"title":"keepalived应用","url":"/2022/04/05/keepalived应用/","content":"\n\n\n\n\n\n\n#### keepalived介绍\n\nKeepalived软件起初是专为LVS负载均衡软件设计的，用来管理并监控LVS集群系统中各个服务节点的状态，后来又加入了可以实现高可用的VRRP功能。因此，Keepalived除了能够管理LVS软件外，还可以作为其他服务（例如：Nginx、Haproxy、MySQL等）的高可用解决方案软件。\n\n#### keepalived的功能\n\n1.实现对lvs集群的高可用和后台节点的服务状态的监控。\n\n2.作为网络服务的高可用，如MySQL,Nginx等。\n\n本片文章主要讲的是软件服务故障切换的高可用。\n\n#### keepalived工作原理\n\nKeepalived高可用服务对之间的故障切换转移，是通过VRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议）来实现的。\n\n在Keepalived服务正常工作时，主Master节点会不断地向备节点发送数据包，用以告诉备Backup节点自己还活着，当主Master节点发生故障时，就无法发送数据包，备节点也就因此无法继续检测到来自主Master节点的心跳了，于是调用自身的接管程序，接管主Master节点的IP资源及服务。而当主Master节点恢复时，备Backup节点又会释放主节点故障时自身接管的IP资源及服务，恢复到原来的备用角色。\n\n#### keepalived+Nginx高可用实战\n\n步骤：\n\n1.准备两台主机\n\nip:192.168.147.7(主)\n\nip:192.168.147.8(备)\n\n2.安装Nginx（安装相对简单不做详述）\n\n3.安装keepalied\n\n4.配置keepalived配置文件实现高可用\n\n5.模拟故障转换\n\n#### keepalived安装\n\n卸载旧的安装包\n\n```shell\n[root@localhost man8]# yum remove keepalived\n```\n\n去官网下载keepalived的安装包https://www.keepalived.org/download.html\n\n开始安装，注意主从服务器都要安装\n\n```shell\n[root@localhost opt]# tar zxvf keepalived-2.2.0.tar.gz            #解压\n[root@localhost opt]# cd keepalived-2.2.0                         #进入解压文件\n[root@localhost keepalived-2.2.0]# ./configure --sysconf=/etc     #编译\n[root@localhost keepalived-2.2.0]# make && make install           #安装\n[root@localhost keepalived-2.2.0]# systemctl start keepalived     #启动\n```\n\n#### keepalived配置\n\n一般配置文件在/etc/keepalived中\n\n```shell\n#主配置文件\n! Configuration File for keepalived\n \n  global_defs {\n     router_id lab1             #标识信息，一个名字而已\n  }  \n     \n  vrrp_instance VI_1 {\n      state MASTER\t\t        #角色是master\n      interface ens33           #vip绑定端口\n      virtual_router_id 51      #让master和backup在同一个虚拟路由中，id号必须一样\n      priority 95               #优先级，谁高，谁就是master\n      advert_int 1              #心跳间隔时间\n      authentication {\n          auth_type PASS        #认证\n          auth_pass 1111        #密码\n      }\n      virtual_ipaddress {\n          192.168.147.16        #虚拟ip\n      }   \n  }\n \n \n #从的配置文件\n ! Configuration File for keepalived\n \n  global_defs {\n     router_id lab2             #标识信息，一个名字而已\n  }  \n     \n  vrrp_instance VI_1 {\n      state BACKUP\t\t        #角色是backup\n      interface ens33           #vip绑定端口\n      virtual_router_id 51      #让master和backup在同一个虚拟路由中，id号必须一样\n      priority 91               #优先级，谁高，谁就是master\n      advert_int 1              #心跳间隔时间\n      authentication {\n          auth_type PASS        #认证\n          auth_pass 1111        #密码\n      }\n      virtual_ipaddress {\n          192.168.147.16        #虚拟ip\n      }   \n  }\n```\n\n#### 故障切换\n\n1.分别进入7和8的页面路径/nginx/html，修改默认访问页面，用来区分访问的是那个服务器的nginx.\n\n![1649321321443](/keepalived应用/1649321321443.png)\n\n![1649321805700](/keepalived应用/1649321805700.png)\n\n2.手动关闭主节点的keepalived模拟故障，可以发现用vip访问的时候是从节点的界面\n\n![1649322111992](/keepalived应用/1649322111992.png)\n\n#### 健康检查脚本\n\n目的：检测nginx挂掉时，尝试重新启动，如果无法启动则停掉keepalived，实现虚拟ip转移\n\n脚本如下:\n\n```shell\n#!/bin/sh\n#Author:hw                  \nnginxpid=$(ps -C nginx --no-header|wc -l)\nif [ $nginxpid -eq 0 ]\n        then\n         /usr/local/nginx2/sbin/nginx\n         echo \"finish\" > /opt/nginx.log\n         sleep 3\n         nginxpid=$(ps -C nginx --no-header|wc -l)\n         if [ $nginxpid -eq 0 ]\n         then\n          systemctl stop keepalived\n         fi\nfi\n```\n\n在keepalived的配置如下：\n\n```shell\n! Configuration File for keepalived\nglobal_defs {\n     script_user root\n     enable_script_security\n     router_id lab1             #标识信息，一个名字而已\n  }\nvrrp_script check {\n      script \"/opt/nginx.sh\"\n      interval 2\n}\n  vrrp_instance VI_1 {\n      state MASTER                      #角色是master\n      interface ens33           #vip绑定端口\n      virtual_router_id 51      #让master和backup在同一个虚拟路由中，id号必须一样\n      priority 95               #优先级，谁高，谁就是master\n      advert_int 1              #心跳间隔时间\n      authentication {\n          auth_type PASS        #认证\n          auth_pass 1111        #密码\n      }\n      virtual_ipaddress {\n          192.168.147.16        #虚拟ip\n      }\n      track_script {\n      check\n      }\n  }\n```\n\n","tags":["keepalived"]},{"title":"shell基础","url":"/2022/04/03/shell/","content":"\n\n\n#### shell脚本的$符号用法\n\n```shell\n#表示第几个参数\n$1,$2...$9\n#表示脚本本身\n$0\n#表示传递的所有参数\n$*\n#传递参数的个数\n$#\n```\n\n![1649385624130](/shell/1649385624130.png)\n\n\n\n\n\n#### 重定向符号:> ,>>,<,<<\n\n```shell\n#> 重定向输入 覆盖原来的数据\n#>> 重定向追加输入 不覆盖\n#< 重定向输出\n#<< 重定向追加输出\n\n#案例,创建分区\nfdisk /dev/sdb <<EOF\nn\np\n3\n\n+1000M\nw\nEOF\n```\n\n#### 数学运算:expr,let,bc,$(())\n\n```shell\n#expr只能进行正数运算\nexpr 1 + 2\nexpr 1 - 2\nexpr 1 \\* 2\nexpr 1 / 2\nlet sum=1+1\necho \"当前内存使用率：`\"scale=2;141*100/7777\" | bc`%\"\n#定义退出的数值\nexit 23 \n$(( 1+2 ))\n```\n\n#### 格式化输出\n\n```shell\necho -n 不换行输出\necho -e 输出内容转义如 \\t,\\a,\\n\n```\n\n#### 基本输入\n\n```shell\nread -s -t50 -n6  a\n#选项说明\n-s 不显示输入，用于密码\n-t 等待时间5s\n-n 只能输入6位\necho $a\n```\n\n#### 变量\n\n```shell\n#定义变量名\n变量名=变量值\n#获取变量$\n$name\n#取消变量\nunset name\n#设置全局变量\nexport name = \"wen\"\n```\n\n#### 数组\n\n```shell\n#数组定义\n数组名 = (1 2 3 4 5 \"q\" )\n#读取数组\necho ${数组名[2]}\n#数组赋值\n数组名[2]= \"g\" \n#打印所有元素\necho ${数组名[@]}\n#统计数组有多少个数\necho ${#数组名[@]}\n#获取数组的索引\necho ${!arry[@]}\n#从数组下表1开始\necho ${arry[@]:1}\n#从数组下表1开始，访问两个元素\necho ${arry[@]:1:2}\n#声明关联数组\ndeclare -A ass_array\n#关联数组赋值\nass_array=([name]='wen' [age]=18)\n```\n\n#### 数据比较运算\n\n```shell\n#数学比较运算\n-eq 等于\n-gt 大于\n-lt 小于\n-ne 不等于\n-ge 大于等于\n-le 小于等于\n#如何处理小数\necho `\"1.5*10\" | bc | cut -d \".\" -f1`\n#if语法格式\n```\n\n#### 文件类型判断\n\n```shell\n#-d 检查目录是否存在\ntest -d /tmp/en; $?\n#-e 检查文件是否存在，判断文件和文件夹\ntest -e /tmp/en ;echo $?\n#-f 判断文件是否存在\n\n#-r 检查文件是否存在且可读\n\n#-s 检查文件是否存在，且不为空\n\n#-w 检查文件是否存在且写\n\n#-x 检查文件是否存在且可执行\n\n#-O 检查文件是否存且当前用户拥有\n\n#-G 检查文件是否存在且默认为当前用户组\n\n#file1 -nt file2 检查file1是否比file2新\n\n#file1 -ot file2 检查file1是否比file2旧\n```\n\n#### 字符串比较运算\n\n```shell\n# == 等于\ntest $USER == 'root';echo $?\n# != 不等于\n# -n 检查字符串的长度是否大于0\n# -z 检查字符串的长度是否为0\n```\n\n#### 逻辑运算\n\n```shell\n#与 &&\n[wen@localhost /]$ if [ 1 == 1 ] && [ 2 == 3 ];then echo $? ;else echo 'n'; fi\nn\n#或 ||\n#非 ！\n```\n\n#### if语法\n\n```shell\nif [ condition ]\nthen\n\nelif\n\nelse\n\nfi\n\n#案例1\n#！/bin/bash\n#判断文件夹是否存在，不存在则创建\nif [ ! -d /tmp/wen  ]\n\tthen\n\t mkdir -v /tmp/wen\n\t echo 'finish'\nfi\n\n#案例2\n#!/bin/bash\nif (( 100%3+1>10 ))\n\tthen\n      echo \"y\"\n    else\n      echo \"n\"\nfi\n\n#案例3\n#!/bin/bash\nfor i in r1 r2 r3 c3\n\tdo\n\t\tif [[ $i ~ r* ]]\n\t\t then\n\t\t  echo $i\n\t\tfi\ndone\n```\n\n#### for循环语句\n\n```shell\n#语法格式\nfor i in var1 var2\ndo\necho $i\ndone\n\n#案例\nfor i in `seq 1 9`\ndo\necho $i\nsleep 1 #停止一秒\ndone\n\nfor i in (( i=1;i<10;i++ ))\ndo\necho $i\ndone\n```\n\n#### 循环控制语句\n\n```shell\n#跳出本次循环\ncontinue\n#跳出整个循环\nbreak\n```\n\n#### while循环\n\nwhile与for的区别，当你明确要循环的次数时就使用for，否则就使用while\n\n```shell\n#语法\nwhile [ condition ]\ndo\n commands\ndone\n\n#案例\nwhile read i\ndo\n echo $i\n done < $1\n```\n\n#### until循环\n\n与while相反，当条件为假的时候执行，为真时停止执行\n\n```shell\n#语法\nuntil [ condition ]\ndo\n\tcommands\ndone\n```\n\n#### case语句\n\n```shell\n#语法格式\ncase $1 in\nvar1)\n\tcommand\n;;\nvar2)\n\tcommand\n;;\n.\n.\n.\n*)\n\tcommand\n;;\nesac\n\n```\n\n#### shell 函数\n\n优点：\n\n1.代码模块化，调用方便，节约内存\n\n2.代码模块化，代码量少，排错简单\n\n3.可以改变代码的执行顺序\n\n```shell\n#函数的语法\n函数名 () {\n    代码块\n    return N\n}\n\n#函数的调用\n函数名\n```\n\n","tags":["shell"]},{"title":"shell脚本","url":"/2022/04/03/shell脚本/","content":"\n#### 服务器配置初始化\n\n```shell\n#!/bin/sh\n#设置时区并同步时间\nln -s /usr/share/zoneinfo/Asia/shanghai /etc/localtime\nif ! crontab -l | grep ntpdate &>/dev/null ; then\n   (echo \"* 1 * * * ntpdate time.windows.com >/dev/null 2>&1\";crontab -l) | crontab\nfi\n#禁用selinux\nsed -i '/SELINUX/{s/permissive/disabled/}' /etc/selinux/config\n#关闭防火墙\nif egrep \"7.[0-9]\" /etc/redhat-release &>/dev/null;then\n\tsystemctl stop firewalld\n\tsystemctl disable firewalld\nelif egrep \"6.[0-9]\" /etc/redhat-release &>/dev/null; then\n\tservice iptables stop\n\tchkconfig iptables off\nfi\n#历史命令显示操作时间\nif ！ grep HISTTIMEFORMAT /etc/bashrc; then\n\techo 'export HISTTIMEFORMAT=\"%F %T `whoami`\"' >> /etc/bashrc\nfi\n#SSH超时时间\nif ! grep \"TMOUT=600\" /etc/profile &>/dev/null; then\n\techo \"export TMOUT=600\" >> /etc/profile\nfi\n#禁止root远程登录\nsed -i 's/#PermitRootLogin yes/PermitRootLogin no/' /etc/ssh/sshd_config\n#禁止定时任务向发送邮件\nsed -i  's/^MAILTO=root/MAILTO=\"\"/' /etc/crontab\n#设置最大打开文件数\nif ! grep \"* soft nofile 65535\" /etc/security/limits.conf &>/dev/null; then\n\tcat >> /etc/security/limits.conf << EOF\n\t* soft nofile 65535\n\t* hard nofile 65535\nEOF\nfi\n#系统内核优化\ncat >> /etc/sysctl.conf << EOF\nnet.ipv4.tcp_syncookies = 1\nnet.ipv4.tcp_max_tw_buckets = 20480\nnet.ipv4.tcp_max_syn_backlog = 20480\nnet.core.netdev_max_backlog = 262144\nnet.ipv4.tcp_fin_timeout =20\nEOF\n#减少SWAP使用\necho \"0\" > /proc/sys/vm/swappiness\n#安装系统性能分析工具及其他\nyum -y install gcc make autoconf vim sysstat net-tools iostat iftop iotp lrzsz\n```\n\n#### 批量创建用户\n\n```shell\n#!/bin/bash\nUSER_LIST=$@\nUSER_FILE=user.txt\nfor USER in $USER_LIST; do\n    if ! id $USER &>/dev/null; then\n        PASS=$(echo $RANDOM |md5sum |cut -c 1-8)\n        useradd $USER\n        echo $PASS |passwd --stdin $USER &>/dev/null\n        echo \"$USER   $PASS\" >> $USER_FILE\n        echo \"$USER User create successful.\"\n    else\n        echo \"$USER User already exists!\"\n    fi\ndone\n```\n\n\n\n#### 一键查看服务器资源利用率\n\n```shell\n#!/bin/bash\nfunction cpu() {\n    NUM=1\n    while [ $NUM -le 3 ]; do\n        util=`vmstat |awk '{if(NR==3)print 100-$15\"%\"}'`\n        user=`vmstat |awk '{if(NR==3)print $13\"%\"}'`\n        sys=`vmstat |awk '{if(NR==3)print $14\"%\"}'`\n        iowait=`vmstat |awk '{if(NR==3)print $16\"%\"}'`\n        echo \"CPU - 使用率: $util , 等待磁盘IO响应使用率: $iowait\"\n        let NUM++\n        sleep 1\n    done\n}\n\nfunction memory() {\n    total=`free -m |awk '{if(NR==2)printf \"%.1f\",$2/1024}'`\n    used=`free -m |awk '{if(NR==2) printf \"%.1f\",($2-$NF)/1024}'`\n    available=`free -m |awk '{if(NR==2) printf \"%.1f\",$NF/1024}'`\n    echo \"内存 - 总大小: ${total}G , 使用: ${used}G , 剩余: ${available}G\"\n}\n\nfunction disk() {\n    fs=$(df -h |awk '/^\\/dev/{print $1}')\n    for p in $fs; do\n        mounted=$(df -h |awk '$1==\"'$p'\"{print $NF}')\n        size=$(df -h |awk '$1==\"'$p'\"{print $2}')\n        used=$(df -h |awk '$1==\"'$p'\"{print $3}')\n        used_percent=$(df -h |awk '$1==\"'$p'\"{print $5}')\n        echo \"硬盘 - 挂载点: $mounted , 总大小: $size , 使用: $used , 使用率: $used_percent\"\n    done\n}\n\nfunction tcp_status() {\n    summary=$(ss -antp |awk '{status[$1]++}END{for(i in status) printf i\":\"status[i]\" \"}')\n    echo \"TCP连接状态 - $summary\"\n}\n\ncpu\nmemory\ndisk\ntcp_status\n```\n\n\n\n#### 统计cpu占比最高的进程\n\n```\nps -eo user,pid,pcpu,pmem,args --sort=-pcpu  |head -n 10\n```\n\n","tags":["shell"]}]